# -*- coding: utf-8 -*-
"""main 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JUI0xGZn6DmGXgPpOP41r1zrTbfQ6X5c
"""

from __future__ import annotations
from typing import TypedDict,List

from langgraph.graph import StateGraph, END
from langchain_groq import ChatGroq
from langgraph.prebuilt import ToolNode

from langchain_core.messages import HumanMessage, AIMessage, BaseMessage,ToolMessage

import json
import ast
from typing import Any


from typing import TypedDict, List, Dict, Any, Optional
from dataclasses import dataclass
import math

from langgraph.graph import StateGraph, END

class ScrumState(TypedDict, total=False):

    cahier_de_charge: str
    team: Dict[str, Any]
    validation_attempts: int
    max_validation_attempts: int

    cleaned_spec: str
    spec_cleaning: Dict[str, Any]      # info about cleaning steps
    spec_validation: Dict[str, Any]    # ok/errors

    spec_fix_suggestions: dict | None
    spec_fix_mode: str | None   # "waiting_user" | "auto_apply"
    enhanced_spec: str | None

    requirements: List[Dict[str, Any]]
    product_backlog: List[Dict[str, Any]]
    refined_backlog: List[Dict[str, Any]]
    estimated_backlog: List[Dict[str, Any]]
    dependencies: List[Dict[str, Any]]

    sprint_backlogs: List[Dict[str, Any]]
    assignments: List[Dict[str, Any]]

    validation: Dict[str, Any]

class ChosenFix(TypedDict):
    error_code: str
    severity: str
    message: str
    evidence: str
    fix_id: str
    title: str
    paragraph: str



class SpecMergeState(TypedDict, total=False):
    spec_original: str
    chosen_fixes: List[ChosenFix]
    manual_patch: str
    spec_enhanced: str
    merge_report: Dict[str, Any]

import json
from typing import Dict, Any
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage
model="llama-3.1-8b-instant"
llm = ChatGroq(
    model=model,
    temperature=0
)

def llm_json(prompt: str) -> Dict[str, Any]:
    response = llm.invoke([
        HumanMessage(content=prompt + "\n\nReturn ONLY valid JSON. No markdown.")
    ])

    text = response.content.strip()


    # Optional: handle ```json blocks
    print("old text is:"+text)
    if text.find("```json")!=-1:
        start_index = text.find("```json")
        end_index = text.find("```", start_index + 7)
        text = text[start_index + 7:end_index]
        print("new text is:"+text)
    elif text.find("```")!=-1:
        start_index = text.find("```")
        end_index = text.find("```", start_index + 3)
        text = text[start_index + 3:end_index]
        print("new text is:"+text)

    return parse_llm_json_or_python_dict(text)
def llm_text(prompt: str) -> str:
    """
    Returns plain text from the LLM.
    No JSON parsing.
    """
    resp = llm.invoke(prompt)
    return resp.content.strip()

import json, ast, re

def parse_llm_json_or_python_dict(text: str):
    text = text.strip()

    # 1) try JSON first
    try:
        return json.loads(text)
    except Exception:
        pass

    # 2) try python dict safely
    try:
        return ast.literal_eval(text)
    except Exception:
        pass

    m = re.search(r"\{.*\}", text, re.DOTALL)
    if m:
        candidate = m.group(0)
        try:
            return json.loads(candidate)
        except Exception:
            try:
                return ast.literal_eval(candidate)
            except Exception:
                pass

    return None

def clean_specifications_llm_node(state: ScrumState) -> ScrumState:

    raw_spec = (state.get("cahier_de_charge") or "").strip()

    prompt = f"""
You are a senior business analyst.

Task:
Rewrite the following specification into a clean, structured version.

Rules:
- Keep the meaning the same
- Remove duplication
- Fix unclear sentences
- Output MUST be plain text (not JSON)
- Use this structure:

TITLE:
SCOPE:
ACTORS:
FUNCTIONAL REQUIREMENTS:
NON-FUNCTIONAL REQUIREMENTS:
CONSTRAINTS:
OUT OF SCOPE:
OPEN QUESTIONS:

Specification:
{raw_spec}
""".strip()

    resp = llm.invoke([HumanMessage(content=prompt)])

    state["cleaned_spec"] = resp.content.strip()
    state["spec_cleaning"] = {
        "ok": True,
        "method": "llm_rewrite",
        "model": model
    }
    print("cleaned_spec is:"+state["cleaned_spec"])
    return state

def validate_specifications_llm_node(state: ScrumState) -> ScrumState:

    spec = (state.get("cahier_de_charge") or state.get("cahier_de_charge") or "").strip()

    if not spec:
        state["spec_validation"] = {
            "ok": False,
            "errors": [{
                "code": "OTHER",
                "message": "Le cahier des charges est vide.",
                "severity": "high",
                "hint": "Veuillez fournir un cahier des charges."
            }]
        }
        return state

    prompt = f"""
Tu es un auditeur de cahier des charges.

TÂCHE:
Faire une vérification LÉGÈRE de la qualité du cahier des charges.

CONSIGNES:
- Ne sois PAS trop strict
- Vérifie juste les problèmes ÉVIDENTS et IMPORTANTS
- Si le cahier des charges est globalement acceptable => ok = true
- Ne cherche pas la perfection

TYPES DE PROBLÈMES À DÉTECTER (seulement si VRAIMENT problématiques):
- SCOPE_TOO_VAGUE: Le scope est vraiment trop flou pour être implémenté
- REQUIREMENTS_TOO_HIGH_LEVEL: Les exigences sont beaucoup trop générales
- NFR_TOO_WEAK: Les exigences non-fonctionnelles sont vraiment manquantes
- AMBIGUOUS: Il y a des contradictions claires
- INCONSISTENT: Il y a des incohérences importantes
- OTHER: Autre problème majeur

RÈGLES DE SORTIE:
- Si errors contient au moins 1 élément => ok = false
- Si ok = true => errors = []
- Le champ "message" ne doit JAMAIS être vide
- Renvoie uniquement du JSON valide

SCHEMA JSON:
{{
  "ok": true ou false,
  "errors": [
    {{
      "code": "SCOPE_TOO_VAGUE|REQUIREMENTS_TOO_HIGH_LEVEL|NFR_TOO_WEAK|AMBIGUOUS|INCONSISTENT|OTHER",
      "message": "Description du problème",
      "severity": "low|medium|high",
      "hint": "Conseil pour améliorer"
    }}
  ]
}}

CAHIER DES CHARGES:
{spec}

Réponds UNIQUEMENT avec du JSON valide.
""".strip()

    resp = llm_json(prompt)

    # Light validation: just check structure
    errors = resp.get("errors", [])
    filtered = []

    for e in errors:
        if not isinstance(e, dict):
            continue

        message = (e.get("message") or "").strip()

        # Only requirement: message must not be empty
        if message:
            # Remove evidence field if present
            if "evidence" in e:
                del e["evidence"]
            filtered.append(e)

    resp["errors"] = filtered
    resp["ok"] = (len(filtered) == 0)

    state["spec_validation"] = resp
    return state


def generate_spec_fix_suggestions_node(state: ScrumState) -> ScrumState:
    """
    Génère <= 3 suggestions de correction par erreur de validation.
    Chaque suggestion est un petit paragraphe prêt à afficher dans le frontend.
    """

    spec = (state.get("cleaned_spec") or state.get("cahier_de_charge") or "").strip()
    validation = state.get("spec_validation") or {}
    errors = validation.get("errors", [])

    # Si aucune erreur, rien à faire
    if not errors:
        state["spec_fix_suggestions"] = {"ok": True, "suggestions": []}
        return state

    prompt = f"""
Tu es un auditeur de cahier des charges.

TÂCHE:
Étant donné un cahier des charges (en français) et la liste des erreurs détectées,
les reponses generé doit etre en français
génère des suggestions concrètes pour corriger ces erreurs.

RÈGLES:
- Pour CHAQUE erreur, retourne AU MAXIMUM 3 suggestions.
- Chaque suggestion doit être un petit paragraphe (2 à 5 lignes).
- Les suggestions doivent être concrètes et directement insérables dans le cahier des charges.
- NE PAS inventer de nouvelles fonctionnalités.
- NE PAS réécrire tout le document.
- Si l'erreur est de sévérité faible, fais des suggestions courtes.
- Retourne UNIQUEMENT du JSON valide (guillemets doubles uniquement).

SCHEMA DE SORTIE:
{{
  "ok": true,
  "suggestions": [
    {{
      "error_code": "string",
      "severity": "low|medium|high",
      "message": "string",
      "evidence": "string",
      "fixes": [
        {{
          "id": "S1",
          "title": "string",
          "paragraph": "string"
        }}
      ]
    }}
  ]
}}

ERREURS DE VALIDATION:
{errors}

CAHIER DES CHARGES:
{spec}
""".strip()

    resp = llm_json(prompt)
    state["spec_fix_suggestions"] = resp

    # Le pipeline doit s'arrêter ici pour attendre le choix utilisateur côté frontend
    state["spec_fix_mode"] = "waiting_user"
    return state




def route_after_spec_validation(state: ScrumState) -> str:
    return "valid" if state["spec_validation"].get("ok") else "invalid"

def extract_requirements_node(state: ScrumState) -> ScrumState:
    spec = state.get("cleaned_spec", state["cahier_de_charge"])


    prompt = f"""
Extract requirements from this cahier de charge.

Return JSON with:
requirements: [
  {{
    "id": "R1",
    "type": "functional|nfr",
    "text": "...",
    "priority": "must|should|could",
    "notes": "optional"
  }}
]

SPEC:
{spec}
"""
    out = llm_json(prompt)
    print("requirement out is:"+str(out))
    state["requirements"] = out
    return state


def generate_product_backlog_node(state: ScrumState) -> ScrumState:
    reqs = state["requirements"]

    prompt = f"""
Convert requirements into a Scrum Product Backlog.
please for all the field use " " as separator and don't use ' '

Return JSON with:
product_backlog: [
  {{
    "epic": "Epic name",
    "stories": [
      {{
        "id": "US1",
        "title": "...",
        "as_a": "...",
        "i_want": "...",
        "so_that": "...",
        "acceptance_criteria": ["..."],
        "required_skills": ["backend","frontend","devops","qa"]
      }}
    ]
  }}
]

REQUIREMENTS:
{reqs}
"""
    out = llm_json(prompt)

    #print("productBacklog out is:"+str(out))
    state["product_backlog"] = out["product_backlog"]
    return state


def refine_backlog_node(state: ScrumState) -> ScrumState:
    pb = state["product_backlog"]

    if(state.get("validation", {}).get("ok", False) == False):
        validation_feedback = state.get("validation")
    else:
        validation_feedback = {"ok":True}
    print("validation_feedback is:"+str(validation_feedback))
    prompt = f"""
Refine the backlog using INVEST:
- split stories that are too big
- remove duplicates
- add missing acceptance criteria
Return JSON with:
refined_backlog: [
  {{
    "id": "US1",
    "title": "...",
    "description": "...",
    "acceptance_criteria": ["..."],
    "required_skills": ["backend","frontend","devops","qa"]
  }}
]


IMPORTANT:
- Output ONLY JSON.
- No python code.
- No markdown.
- No explanations.
Validation_feedback:
{validation_feedback}
PRODUCT_BACKLOG:
{pb}
"""
    out = llm_json(prompt)
    print("refined_backlog out is:"+str(out))
    state["refined_backlog"] = out
    return state


def estimate_backlog_node(state: ScrumState) -> ScrumState:
    refined = state["refined_backlog"]
    team = state["team"]

    if(state.get("validation", {}).get("ok", False) == False):
        validation_feedback = state.get("validation")
    else:
        validation_feedback = {"ok":True}
    print("validation_feedback is:"+str(validation_feedback))


    prompt = f"""
Estimate each story using Fibonacci story points: 1,2,3,5,8,13,21.

Return JSON with:
{{
  "estimated_backlog": [
    {{
      "id": "...",
      "title": "...",
      "points": 1|2|3|5|8|13|21,
      "risk": "low|medium|high",
      "complexity": "low|medium|high",
      "required_skills": [...]
    }}
  ]
}}

IMPORTANT:
- Output ONLY JSON.
- No python code.
- No markdown.
- No explanations.

validationFeedback:
{validation_feedback}

TEAM:
{team}

STORIES:
{refined}
"""

    out = llm_json(prompt)
    print("estimated_backlog out is:"+str(out))
    state["estimated_backlog"] = out["estimated_backlog"]
    return state


def map_dependencies_node(state: ScrumState) -> ScrumState:
    stories = state["estimated_backlog"]

    prompt = f"""
Detect dependencies between stories.

Return JSON with:
dependencies: [
  {{
    "from": "US1",
    "to": "US5",
    "type": "blocks"
  }}
]
STORIES:
{stories}
"""
    out = llm_json(prompt)
    print("dependencies out is:"+str(out))
    try:
        state["dependencies"] = out["dependencies"]
    except:
        state["dependencies"] = out
    return state

def sprint_planner_node(state: ScrumState) -> ScrumState:
    stories = state["estimated_backlog"]
    deps = state.get("dependencies", [])
    capacity = int(state["team"].get("sprint_capacity_points", 20))

    # Simple heuristic: order by dependencies first (very simplified)
    # (In production: topological sort)
    ordered = stories[:]  # assume already prioritized by LLM

    sprints = []
    current = {"sprint": 1, "items": [], "total_points": 0}

    for st in ordered:
        pts = int(st["points"])
        if current["total_points"] + pts > capacity and current["items"]:
            sprints.append(current)
            current = {"sprint": current["sprint"] + 1, "items": [], "total_points": 0}

        current["items"].append(st["id"])
        current["total_points"] += pts

    if current["items"]:
        sprints.append(current)

    state["sprint_backlogs"] = sprints
    return state

def contributor_assigner_node(state: ScrumState) -> ScrumState:
    team_members = state["team"]["members"]
    stories = {s["id"]: s for s in state["estimated_backlog"]}

    # Very simple skill matching:
    # assign 1 main person who matches most skills
    assignments = []

    for sprint in state["sprint_backlogs"]:
        for story_id in sprint["items"]:
            story = stories[story_id]
            req_skills = set(story.get("required_skills", []))

            best = None
            best_score = -1

            for m in team_members:
                skills = set(m.get("skills", []))
                score = len(req_skills.intersection(skills))
                if score > best_score:
                    best_score = score
                    best = m

            assignments.append({
                "story_id": story_id,
                "title": story.get("title", ""),
                "assigned_to": best["name"] if best else None,
                "reason": f"matched_skills={best_score}"
            })

    state["assignments"] = assignments
    return state


def validation_node(state: ScrumState) -> ScrumState:
    state["validation_attempts"] = int(state.get("validation_attempts", 0)) + 1
    capacity = int(state["team"].get("sprint_capacity_points", 20))
    sprints = state["sprint_backlogs"]
    stories = {s["id"]: s for s in state["estimated_backlog"]}

    issues = []

    for sp in sprints:
        total = sum(int(stories[sid]["points"]) for sid in sp["items"])
        if total > capacity:
            issues.append({
                "type": "over_capacity",
                "sprint": sp["sprint"],
                "total_points": total,
                "capacity": capacity
            })

    # Example: detect too big stories
    for s in state["estimated_backlog"]:
        if int(s["points"]) >= 13:
            issues.append({
                "type": "story_too_big",
                "story_id": s["id"],
                "points": s["points"]
            })

    state["validation"] = {
        "ok": len(issues) == 0,
        "issues": issues
    }
    return state

def route_after_validation(state: ScrumState) -> str:
    attempts = int(state.get("validation_attempts", 0))
    max_attempts = int(state.get("max_validation_attempts", 3))

    if attempts >= max_attempts and not state["validation"]["ok"]:
        state["validation"]["stopped_reason"] = "Max validation attempts reached"

        return "stop"
    if state["validation"]["ok"]:
        print("validation ok, we can continue")
        return "done"
    # if stories too big -> refine again
    for issue in state["validation"]["issues"]:
        if issue["type"] == "story_too_big":
            print("we will refine because of issue:"+str(issue))
            return "refine"
    # if over capacity -> plan again
    print("we will plan again because of over capacity")
    return "replan"

def merge_spec_with_chosen_fixes_node(state: SpecMergeState) -> SpecMergeState:
    spec = (state.get("spec_original") or "").strip()
    chosen = state.get("chosen_fixes", [])
    manual_patch = (state.get("manual_patch") or "").strip()

    if not spec:
        state["spec_enhanced"] = ""
        state["merge_report"] = {"ok": False, "reason": "spec_original is empty"}
        return state

    if not chosen and not manual_patch:
        state["spec_enhanced"] = spec
        state["merge_report"] = {"ok": True, "applied_fixes": 0, "note": "no changes"}
        return state

    prompt = f"""
Tu es un assistant expert en rédaction de cahier des charges.

OBJECTIF:
Produire une version améliorée du cahier des charges en appliquant UNIQUEMENT les corrections choisies.

RÈGLES STRICTES:
1) Appliquer uniquement les paragraphes fournis dans "CORRECTIONS CHOISIES".
2) Ne pas inventer de nouvelles fonctionnalités.
3) Ne pas supprimer des sections existantes.
4) Intégrer le texte manuel utilisateur en priorité si non vide.
5) Retourner uniquement le texte final (pas de JSON, pas de markdown, pas d'explications).

CORRECTIONS CHOISIES:
{chosen}

TEXTE MANUEL UTILISATEUR:
{manual_patch}

CAHIER DES CHARGES ORIGINAL:
{spec}
""".strip()

    enhanced = llm_text(prompt)

    state["spec_enhanced"] = enhanced
    state["merge_report"] = {
        "ok": True,
        "applied_fixes": len(chosen),
        "manual_patch_used": bool(manual_patch),
    }
    return state

def build_scrum_graph():
    g = StateGraph(ScrumState)
    g.add_node("clean_spec", clean_specifications_llm_node)
    g.add_node("validate_spec", validate_specifications_llm_node)
    g.add_node("extract_requirements", extract_requirements_node)
    g.add_node("generate_product_backlog", generate_product_backlog_node)
    g.add_node("refine_backlog", refine_backlog_node)
    g.add_node("estimate_backlog", estimate_backlog_node)
    g.add_node("map_dependencies", map_dependencies_node)
    g.add_node("sprint_planner", sprint_planner_node)
    g.add_node("contributor_assigner", contributor_assigner_node)
    g.add_node("validation", validation_node)
    g.add_node("spec_suggestions", generate_spec_fix_suggestions_node)


    # Edges
    g.set_entry_point("clean_spec")
    g.add_edge("clean_spec", "validate_spec")

    g.add_conditional_edges(
        "validate_spec",
        route_after_spec_validation,
        {
            "valid": "extract_requirements",
            "invalid": "spec_suggestions"
        }
    )

    g.add_edge("extract_requirements", "generate_product_backlog")
    g.add_edge("generate_product_backlog", "refine_backlog")

    g.add_edge("refine_backlog", "estimate_backlog")
    g.add_edge("estimate_backlog", "map_dependencies")
    g.add_edge("map_dependencies", "sprint_planner")
    g.add_edge("sprint_planner", "contributor_assigner")
    g.add_edge("contributor_assigner", "validation")
    g.add_edge("spec_suggestions", END)

    # Conditional routing after validation
    g.add_conditional_edges(
        "validation",
        route_after_validation,
        {
            "done": END,
            "refine": "estimate_backlog",
            "replan": "sprint_planner",
            "stop": END
        }
    )

    return g.compile()

from langgraph.graph import StateGraph, END

merge_graph = StateGraph(SpecMergeState)
merge_graph.add_node("merge_spec", merge_spec_with_chosen_fixes_node)

merge_graph.set_entry_point("merge_spec")
merge_graph.add_edge("merge_spec", END)

merge_app = merge_graph.compile()

if __name__ == "__main__":
    graph = build_scrum_graph()

    cahier_de_charge = """
Cahier des Charges — Équipe Joy
Projet SummerCamp 2025
PorterVision — Plateforme d’analyse stratégique automatisée

1. **Contexte**
PorterVision est une plateforme moderne qui aide les entreprises à analyser leurs projets avec une méthode stratégique.
Le but est d’améliorer la prise de décision grâce à l’intelligence artificielle.

2. **Objectifs**
- Réduire le temps de prise de décision de 30% grâce à l'analyse automatique des données.
- Générer des recommandations précises et personnalisées avec une précision de 90% (mesurée via des retours utilisateurs).
- Fournir un dashboard clair et facile à utiliser, avec une interface répondant aux critères de Nielsen pour l’ergonomie.

3. **Périmètre**
Les limites de projet doivent être définies pour garantir que la plateforme PorterVision ne dépasse pas les ressources disponibles.
- **Ressources humaines** : Équipe de 5 à 7 personnes (développement, IA, design).
- **Ressources matérielles** : Infrastructure cloud AWS/GCP avec un budget maximal de 500 000 €.
- **Contraintes techniques** : Utilisation exclusive des stacks définis dans les contraintes techniques.

4. **Acteurs**
- Admin
- Utilisateur

5. **Fonctionnalités attendues**
- Upload de fichiers (formats supportés : CSV, Excel, PDF).
- Analyse automatique (traitement en temps réel des données).
- Dashboard (visualisation en temps réel avec filtres dynamiques).
- Génération de rapport (export PDF/Excel).
- Recommandations IA (basées sur des modèles pré-entraînés).

6. **Contraintes techniques**
- Backend : Python ou Java ou NodeJS
- Frontend : React ou Angular ou Vue
- Base de données : PostgreSQL ou MongoDB
- Agents IA : LangChain ou CrewAI ou AutoGen

7. **Exigences non fonctionnelles**
- Disponibilité de 99,9% sur 24/7 avec un temps de réponse inférieur à 2 secondes.
- Sécurité : Conformité RGPD, chiffrement des données en transit et au repos.
- Scalabilité : Support de 10 000 utilisateurs simultanés.

8. **KPI**
- Précision des recommandations IA : 90% (mesurée via des tests A/B).
- Réduction du temps de prise de décision de 30% (mesurée via des études de cas).

9. **Livrables**
- Application web (responsive, multiplateforme).
- API RESTful documentée.
- Rapport PDF (généré automatiquement avec les analyses).

10. **Équipe projet**
L’équipe sera définie plus tard.
"""


    team = {
        "sprint_length_days": 14,
        "sprint_capacity_points": 20,
        "members": [
            {"name": "Sami", "role": "Backend", "skills": ["backend", "sql", "spring"]},
            {"name": "Ali", "role": "Frontend", "skills": ["frontend", "angular", "ui"]},
            {"name": "Mouna", "role": "DevOps", "skills": ["devops", "docker", "ci_cd"]},
            {"name": "Hela", "role": "QA", "skills": ["qa", "testing"]}
        ]
    }

    result = graph.invoke({
        "cahier_de_charge": cahier_de_charge,
        "team": team,
        "validation_attempts": 0,
        "max_validation_attempts": 3
    })

    print(result)
    if(result["spec_validation"]["ok"]==False):
        print("validation failed")
        print(result["spec_validation"]["errors"])
        print(result["spec_fix_suggestions"])
    else:
        print("============================================")
        print("Requirements:", result["requirements"])
        print("============================================")
        print("Product Backlog:", result["product_backlog"])
        print("============================================")
        print("Estimated Backlog:", result["estimated_backlog"])
        print("============================================")
        print("Dependencies:", result.get("dependencies", []))
        print("============================================")
        print("Sprints:", result["sprint_backlogs"])
        print("============================================")
        print("Assignments:", result["assignments"])
        print("============================================")
        print("Validation:", result["validation"])
        print("============================================")

# --- Build chosen_fixes from the suggestion generator output ---
    chosen_fixes = [
        {
            "error_code": "REQUIREMENTS_TOO_HIGH_LEVEL",
            "severity": "medium",
            "message": "Les objectifs sont trop généraux et nécessitent une clarification",
            "evidence": "Les objectifs sont trop généraux et nécessitent une clarification",
            "fix_id": "S1",
            "title": "Définir des objectifs plus spécifiques et mesurables",
            "paragraph": "Les objectifs doivent être définis de manière plus spécifique et mesurable pour garantir que la plateforme PorterVision répond aux besoins réels des utilisateurs. Par exemple, l'objectif pourrait être de 'réduire le temps de prise de décision de 30% grâce à l'analyse automatique des données'."
        },
        {
            "error_code": "NFR_TOO_WEAK",
            "severity": "high",
            "message": "Les exigences non-fonctionnelles sont insuffisantes et nécessitent une complétion",
            "evidence": "Les exigences non-fonctionnelles sont insuffisantes et nécessitent une complétion",
            "fix_id": "S1",
            "title": "Définir des exigences non-fonctionnelles plus détaillées",
            "paragraph": "Les exigences non-fonctionnelles doivent être définies de manière plus détaillée pour garantir que la plateforme PorterVision répond aux besoins réels des utilisateurs. Par exemple, l'exigence non-fonctionnelle pourrait être 'la plateforme doit être accessible en ligne 24/7 avec une disponibilité de 99,9%'."
        },
        {
            "error_code": "OTHER",
            "severity": "medium",
            "message": "Le périmètre du projet est trop vague et nécessite une clarification",
            "evidence": "Le périmètre du projet est trop vague et nécessite une clarification",
            "fix_id": "S2",
            "title": "Définir des limites de projet",
            "paragraph": "Les limites de projet doivent être définies pour garantir que la plateforme PorterVision ne dépasse pas les ressources disponibles. Par exemple, les limites de projet pourraient inclure la définition des ressources humaines, des ressources matérielles, des budgets, etc."
        }
    ]


    result=merge_app.invoke({ "spec_original": cahier_de_charge, "chosen_fixes": chosen_fixes, "manual_patch": "" })
    print("Enhanced spec after merging chosen fixes:"+result["spec_enhanced"])

