# -*- coding: utf-8 -*-
"""main 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JUI0xGZn6DmGXgPpOP41r1zrTbfQ6X5c
"""

from __future__ import annotations
from typing import TypedDict,List

from langgraph.graph import StateGraph, END
from langchain_groq import ChatGroq
from langgraph.prebuilt import ToolNode

from langchain_core.messages import HumanMessage, AIMessage, BaseMessage,ToolMessage
import re
import json
import ast
from typing import Any


from typing import TypedDict, List, Dict, Any, Optional
from dataclasses import dataclass
import math
import re

from langgraph.graph import StateGraph, END

class ScrumState(TypedDict, total=False):

    cahier_de_charge: str
    team: Dict[str, Any]
    validation_attempts: int
    max_validation_attempts: int

    cleaned_spec: str
    spec_cleaning: Dict[str, Any]      # info about cleaning steps
    spec_validation: Dict[str, Any]    # ok/errors

    spec_fix_suggestions: dict | None
    spec_fix_mode: str | None   # "waiting_user" | "auto_apply"
    enhanced_spec: str | None

    requirements: List[Dict[str, Any]]
    product_backlog: List[Dict[str, Any]]
    refined_backlog: List[Dict[str, Any]]
    estimated_backlog: List[Dict[str, Any]]
    dependencies: List[Dict[str, Any]]

    sprint_backlogs: List[Dict[str, Any]]
    assignments: List[Dict[str, Any]]

    validation: Dict[str, Any]

class ChosenFix(TypedDict):
    error_code: str
    severity: str
    message: str
    evidence: str
    fix_id: str
    title: str
    paragraph: str



class SpecMergeState(TypedDict, total=False):
    spec_original: str
    chosen_fixes: List[ChosenFix]
    manual_patch: str
    spec_enhanced: str
    merge_report: Dict[str, Any]

import json
from typing import Dict, Any
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage
model="llama-3.1-8b-instant"
llm = ChatGroq(
    model=model,
    temperature=0
)

def llm_json(prompt: str) -> Dict[str, Any]:
    response = llm.invoke([
        HumanMessage(content=prompt + "\n\nReturn ONLY valid JSON. No markdown.")
    ])

    text = response.content.strip()


    # Optional: handle ```json blocks
    print("old text is:"+text)
    if text.find("```json")!=-1:
        start_index = text.find("```json")
        end_index = text.find("```", start_index + 7)
        text = text[start_index + 7:end_index]
        print("new text is:"+text)
    elif text.find("```")!=-1:
        start_index = text.find("```")
        end_index = text.find("```", start_index + 3)
        text = text[start_index + 3:end_index]
        print("new text is:"+text)

    return parse_llm_json_or_python_dict(text)
def llm_text(prompt: str) -> str:
    """
    Returns plain text from the LLM.
    No JSON parsing.
    """
    resp = llm.invoke(prompt)
    return resp.content.strip()

import json, ast, re

def parse_llm_json_or_python_dict(text: str):
    text = text.strip()

    # 1) try JSON first
    try:
        return json.loads(text)
    except Exception:
        pass

    # 2) try python dict safely
    try:
        return ast.literal_eval(text)
    except Exception:
        pass

    m = re.search(r"\{.*\}", text, re.DOTALL)
    if m:
        candidate = m.group(0)
        try:
            return json.loads(candidate)
        except Exception:
            try:
                return ast.literal_eval(candidate)
            except Exception:
                pass

    return None

def clean_specifications_llm_node(state: ScrumState) -> ScrumState:

    raw_spec = (state.get("cahier_de_charge") or "").strip()

    prompt = f"""
You are a senior business analyst.

Task:
Rewrite the following specification into a clean, structured version.

Rules:
- Keep the meaning the same
- Remove duplication
- Fix unclear sentences
- Output MUST be plain text (not JSON)
- Use this structure:

TITLE:
SCOPE:
ACTORS:
FUNCTIONAL REQUIREMENTS:
NON-FUNCTIONAL REQUIREMENTS:
CONSTRAINTS:
OUT OF SCOPE:
OPEN QUESTIONS:

Specification:
{raw_spec}
""".strip()

    resp = llm.invoke([HumanMessage(content=prompt)])

    state["cleaned_spec"] = resp.content.strip()
    state["spec_cleaning"] = {
        "ok": True,
        "method": "llm_rewrite",
        "model": model
    }
    print("cleaned_spec is:"+state["cleaned_spec"])
    return state

def validate_specifications_llm_node(state: ScrumState) -> ScrumState:

    spec = (state.get("cahier_de_charge") or state.get("cahier_de_charge") or "").strip()

    if not spec:
        state["spec_validation"] = {
            "ok": False,
            "errors": [{
                "code": "OTHER",
                "message": "Le cahier des charges est vide.",
                "severity": "high",
                "hint": "Veuillez fournir un cahier des charges."
            }]
        }
        return state

    prompt = f"""
Tu es un auditeur de cahier des charges.

TÃ‚CHE:
Faire une vÃ©rification LÃ‰GÃˆRE de la qualitÃ© du cahier des charges.

CONSIGNES:
- Ne sois PAS trop strict
- VÃ©rifie juste les problÃ¨mes Ã‰VIDENTS et IMPORTANTS
- Si le cahier des charges est globalement acceptable => ok = true
- Ne cherche pas la perfection

TYPES DE PROBLÃˆMES Ã€ DÃ‰TECTER (seulement si VRAIMENT problÃ©matiques):
- SCOPE_TOO_VAGUE: Le scope est vraiment trop flou pour Ãªtre implÃ©mentÃ©
- REQUIREMENTS_TOO_HIGH_LEVEL: Les exigences sont beaucoup trop gÃ©nÃ©rales
- NFR_TOO_WEAK: Les exigences non-fonctionnelles sont vraiment manquantes
- AMBIGUOUS: Il y a des contradictions claires
- INCONSISTENT: Il y a des incohÃ©rences importantes
- OTHER: Autre problÃ¨me majeur

RÃˆGLES DE SORTIE:
- Si errors contient au moins 1 Ã©lÃ©ment => ok = false
- Si ok = true => errors = []
- Le champ "message" ne doit JAMAIS Ãªtre vide
- Renvoie uniquement du JSON valide

SCHEMA JSON:
{{
  "ok": true ou false,
  "errors": [
    {{
      "code": "SCOPE_TOO_VAGUE|REQUIREMENTS_TOO_HIGH_LEVEL|NFR_TOO_WEAK|AMBIGUOUS|INCONSISTENT|OTHER",
      "message": "Description du problÃ¨me",
      "severity": "low|medium|high",
      "hint": "Conseil pour amÃ©liorer"
    }}
  ]
}}

CAHIER DES CHARGES:
{spec}

RÃ©ponds UNIQUEMENT avec du JSON valide.
""".strip()

    resp = llm_json(prompt)

    # Light validation: just check structure
    errors = resp.get("errors", [])
    filtered = []

    for e in errors:
        if not isinstance(e, dict):
            continue

        message = (e.get("message") or "").strip()

        # Only requirement: message must not be empty
        if message:
            # Remove evidence field if present
            if "evidence" in e:
                del e["evidence"]
            filtered.append(e)

    resp["errors"] = filtered
    resp["ok"] = (len(filtered) == 0)

    state["spec_validation"] = resp
    return state


def generate_spec_fix_suggestions_node(state: ScrumState) -> ScrumState:
    """
    GÃ©nÃ¨re <= 3 suggestions de correction par erreur de validation.
    Chaque suggestion est un petit paragraphe prÃªt Ã  afficher dans le frontend.
    """

    spec = (state.get("cleaned_spec") or state.get("cahier_de_charge") or "").strip()
    validation = state.get("spec_validation") or {}
    errors = validation.get("errors", [])

    # Si aucune erreur, rien Ã  faire
    if not errors:
        state["spec_fix_suggestions"] = {"ok": True, "suggestions": []}
        return state

    prompt = f"""
Tu es un auditeur de cahier des charges.

TÃ‚CHE:
Ã‰tant donnÃ© un cahier des charges (en franÃ§ais) et la liste des erreurs dÃ©tectÃ©es,
les reponses generÃ© doit etre en franÃ§ais
gÃ©nÃ¨re des suggestions concrÃ¨tes pour corriger ces erreurs.

RÃˆGLES:
- Pour CHAQUE erreur, retourne AU MAXIMUM 3 suggestions.
- Chaque suggestion doit Ãªtre un petit paragraphe (2 Ã  5 lignes).
- Les suggestions doivent Ãªtre concrÃ¨tes et directement insÃ©rables dans le cahier des charges.
- NE PAS inventer de nouvelles fonctionnalitÃ©s.
- NE PAS rÃ©Ã©crire tout le document.
- Si l'erreur est de sÃ©vÃ©ritÃ© faible, fais des suggestions courtes.
- Retourne UNIQUEMENT du JSON valide (guillemets doubles uniquement).

SCHEMA DE SORTIE:
{{
  "ok": true,
  "suggestions": [
    {{
      "error_code": "string",
      "severity": "low|medium|high",
      "message": "string",
      "evidence": "string",
      "fixes": [
        {{
          "id": "S1",
          "title": "string",
          "paragraph": "string"
        }}
      ]
    }}
  ]
}}

ERREURS DE VALIDATION:
{errors}

CAHIER DES CHARGES:
{spec}
""".strip()

    resp = llm_json(prompt)
    state["spec_fix_suggestions"] = resp

    # Le pipeline doit s'arrÃªter ici pour attendre le choix utilisateur cÃ´tÃ© frontend
    state["spec_fix_mode"] = "waiting_user"
    return state




def route_after_spec_validation(state: ScrumState) -> str:
    return "valid" if state["spec_validation"].get("ok") else "invalid"

def extract_requirements_node(state: ScrumState) -> ScrumState:
    spec = state.get("cleaned_spec", state["cahier_de_charge"])


    prompt = f"""
Extract requirements from this cahier de charge.

Return JSON with:
requirements: [
  {{
    "id": "R1",
    "type": "functional|nfr",
    "text": "...",
    "priority": "must|should|could",
    "notes": "optional"
  }}
]

SPEC:
{spec}
"""
    out = llm_json(prompt)
    print("requirement out is:"+str(out))
    state["requirements"] = out
    return state


def generate_product_backlog_node(state: ScrumState) -> ScrumState:
    reqs = state["requirements"]

    prompt = f"""
Convert requirements into a Scrum Product Backlog.
please for all the field use " " as separator and don't use ' '

Return JSON with:
product_backlog: [
  {{
    "epic": "Epic name",
    "stories": [
      {{
        "id": "US1",
        "title": "...",
        "as_a": "...",
        "i_want": "...",
        "so_that": "...",
        "acceptance_criteria": ["..."],
        "required_skills": ["backend","frontend","devops","qa"]
      }}
    ]
  }}
]

REQUIREMENTS:
{reqs}
"""
    out = llm_json(prompt)

    #print("productBacklog out is:"+str(out))
    state["product_backlog"] = out["product_backlog"]
    return state


def refine_backlog_node(state: ScrumState) -> ScrumState:
    pb = state["product_backlog"]

    if(state.get("validation", {}).get("ok", False) == False):
        validation_feedback = state.get("validation")
    else:
        validation_feedback = {"ok":True}
    print("validation_feedback is:"+str(validation_feedback))
    prompt = f"""
Refine the backlog using INVEST:
- split stories that are too big
- remove duplicates
- add missing acceptance criteria
Return JSON with:
refined_backlog: [
  {{
    "id": "US1",
    "title": "...",
    "description": "...",
    "acceptance_criteria": ["..."],
    "required_skills": ["backend","frontend","devops","qa"]
  }}
]


IMPORTANT:
- Output ONLY JSON.
- No python code.
- No markdown.
- No explanations.
Validation_feedback:
{validation_feedback}
PRODUCT_BACKLOG:
{pb}
"""
    out = llm_json(prompt)
    print("refined_backlog out is:"+str(out))
    state["refined_backlog"] = out
    return state


def estimate_backlog_node(state: ScrumState) -> ScrumState:
    refined = state["refined_backlog"]
    team = state["team"]

    if(state.get("validation", {}).get("ok", False) == False):
        validation_feedback = state.get("validation")
    else:
        validation_feedback = {"ok":True}
    print("validation_feedback is:"+str(validation_feedback))


    prompt = f"""
Estimate each story using Fibonacci story points: 1,2,3,5,8,13,21.

Return JSON with:
{{
  "estimated_backlog": [
    {{
      "id": "...",
      "title": "...",
      "points": 1|2|3|5|8|13|21,
      "risk": "low|medium|high",
      "complexity": "low|medium|high",
      "required_skills": [...]
    }}
  ]
}}

IMPORTANT:
- Output ONLY JSON.
- No python code.
- No markdown.
- No explanations.

validationFeedback:
{validation_feedback}

TEAM:
{team}

STORIES:
{refined}
"""

    out = llm_json(prompt)
    print("estimated_backlog out is:"+str(out))
    state["estimated_backlog"] = out["estimated_backlog"]
    return state


def map_dependencies_node(state: ScrumState) -> ScrumState:
    stories = state["estimated_backlog"]

    prompt = f"""
Detect dependencies between stories.

Return JSON with:
dependencies: [
  {{
    "from": "US1",
    "to": "US5",
    "type": "blocks"
  }}
]
STORIES:
{stories}
"""
    out = llm_json(prompt)
    print("dependencies out is:"+str(out))
    try:
        state["dependencies"] = out["dependencies"]
    except:
        state["dependencies"] = out
    return state

def sprint_planner_node(state: ScrumState) -> ScrumState:
    stories = state["estimated_backlog"]
    deps = state.get("dependencies", [])
    capacity = int(state["team"].get("sprint_capacity_points", 20))

    # Simple heuristic: order by dependencies first (very simplified)
    # (In production: topological sort)
    ordered = stories[:]  # assume already prioritized by LLM

    sprints = []
    current = {"sprint": 1, "items": [], "total_points": 0}

    for st in ordered:
        pts = int(st["points"])
        if current["total_points"] + pts > capacity and current["items"]:
            sprints.append(current)
            current = {"sprint": current["sprint"] + 1, "items": [], "total_points": 0}

        current["items"].append(st["id"])
        current["total_points"] += pts

    if current["items"]:
        sprints.append(current)

    state["sprint_backlogs"] = sprints
    return state

def contributor_assigner_node(state: ScrumState) -> ScrumState:
    team_members = state["team"]["members"]
    stories = {s["id"]: s for s in state["estimated_backlog"]}

    # Very simple skill matching:
    # assign 1 main person who matches most skills
    assignments = []

    for sprint in state["sprint_backlogs"]:
        for story_id in sprint["items"]:
            story = stories[story_id]
            req_skills = set(story.get("required_skills", []))

            best = None
            best_score = -1

            for m in team_members:
                skills = set(m.get("skills", []))
                score = len(req_skills.intersection(skills))
                if score > best_score:
                    best_score = score
                    best = m

            assignments.append({
                "story_id": story_id,
                "title": story.get("title", ""),
                "assigned_to": best["name"] if best else None,
                "reason": f"matched_skills={best_score}"
            })

    state["assignments"] = assignments
    return state


def validation_node(state: ScrumState) -> ScrumState:
    state["validation_attempts"] = int(state.get("validation_attempts", 0)) + 1
    capacity = int(state["team"].get("sprint_capacity_points", 20))
    sprints = state["sprint_backlogs"]
    stories = {s["id"]: s for s in state["estimated_backlog"]}

    issues = []

    for sp in sprints:
        total = sum(int(stories[sid]["points"]) for sid in sp["items"])
        if total > capacity:
            issues.append({
                "type": "over_capacity",
                "sprint": sp["sprint"],
                "total_points": total,
                "capacity": capacity
            })

    # Example: detect too big stories
    for s in state["estimated_backlog"]:
        if int(s["points"]) >= 13:
            issues.append({
                "type": "story_too_big",
                "story_id": s["id"],
                "points": s["points"]
            })

    state["validation"] = {
        "ok": len(issues) == 0,
        "issues": issues
    }
    return state

def route_after_validation(state: ScrumState) -> str:
    attempts = int(state.get("validation_attempts", 0))
    max_attempts = int(state.get("max_validation_attempts", 3))

    if attempts >= max_attempts and not state["validation"]["ok"]:
        state["validation"]["stopped_reason"] = "Max validation attempts reached"

        return "stop"
    if state["validation"]["ok"]:
        print("validation ok, we can continue")
        return "done"
    # if stories too big -> refine again
    for issue in state["validation"]["issues"]:
        if issue["type"] == "story_too_big":
            print("we will refine because of issue:"+str(issue))
            return "refine"
    # if over capacity -> plan again
    print("we will plan again because of over capacity")
    return "replan"

def merge_spec_with_chosen_fixes_node(state: SpecMergeState) -> SpecMergeState:
    spec = (state.get("spec_original") or "").strip()
    chosen = state.get("chosen_fixes", [])
    manual_patch = (state.get("manual_patch") or "").strip()

    if not spec:
        state["spec_enhanced"] = ""
        state["merge_report"] = {"ok": False, "reason": "spec_original is empty"}
        return state

    if not chosen and not manual_patch:
        state["spec_enhanced"] = spec
        state["merge_report"] = {"ok": True, "applied_fixes": 0, "note": "no changes"}
        return state

    prompt = f"""
Tu es un assistant expert en rÃ©daction de cahier des charges.

OBJECTIF:
Produire une version amÃ©liorÃ©e du cahier des charges en appliquant UNIQUEMENT les corrections choisies.

RÃˆGLES STRICTES:
1) Appliquer uniquement les paragraphes fournis dans "CORRECTIONS CHOISIES".
2) Ne pas inventer de nouvelles fonctionnalitÃ©s.
3) Ne pas supprimer des sections existantes.
4) IntÃ©grer le texte manuel utilisateur en prioritÃ© si non vide.
5) Retourner uniquement le texte final (pas de JSON, pas de markdown, pas d'explications).

CORRECTIONS CHOISIES:
{chosen}

TEXTE MANUEL UTILISATEUR:
{manual_patch}

CAHIER DES CHARGES ORIGINAL:
{spec}
""".strip()

    enhanced = llm_text(prompt)

    state["spec_enhanced"] = enhanced
    state["merge_report"] = {
        "ok": True,
        "applied_fixes": len(chosen),
        "manual_patch_used": bool(manual_patch),
    }
    return state

def build_scrum_graph():
    g = StateGraph(ScrumState)
    g.add_node("clean_spec", clean_specifications_llm_node)
    g.add_node("validate_spec", validate_specifications_llm_node)
    g.add_node("extract_requirements", extract_requirements_node)
    g.add_node("generate_product_backlog", generate_product_backlog_node)
    g.add_node("refine_backlog", refine_backlog_node)
    g.add_node("estimate_backlog", estimate_backlog_node)
    g.add_node("map_dependencies", map_dependencies_node)
    g.add_node("sprint_planner", sprint_planner_node)
    g.add_node("contributor_assigner", contributor_assigner_node)
    g.add_node("validation", validation_node)
    g.add_node("spec_suggestions", generate_spec_fix_suggestions_node)


    # Edges
    g.set_entry_point("clean_spec")
    g.add_edge("clean_spec", "validate_spec")

    g.add_conditional_edges(
        "validate_spec",
        route_after_spec_validation,
        {
            "valid": "extract_requirements",
            "invalid": "spec_suggestions"
        }
    )

    g.add_edge("extract_requirements", "generate_product_backlog")
    g.add_edge("generate_product_backlog", "refine_backlog")

    g.add_edge("refine_backlog", "estimate_backlog")
    g.add_edge("estimate_backlog", "map_dependencies")
    g.add_edge("map_dependencies", "sprint_planner")
    g.add_edge("sprint_planner", "contributor_assigner")
    g.add_edge("contributor_assigner", "validation")
    g.add_edge("spec_suggestions", END)

    # Conditional routing after validation
    g.add_conditional_edges(
        "validation",
        route_after_validation,
        {
            "done": END,
            "refine": "estimate_backlog",
            "replan": "sprint_planner",
            "stop": END
        }
    )

    return g.compile()

from langgraph.graph import StateGraph, END

merge_graph = StateGraph(SpecMergeState)
merge_graph.add_node("merge_spec", merge_spec_with_chosen_fixes_node)

merge_graph.set_entry_point("merge_spec")
merge_graph.add_edge("merge_spec", END)

merge_app = merge_graph.compile()


def extract_project_title(cahier: str) -> str:
    """
    Extrait le titre du projet depuis un cahier de charges.
    Cherche une ligne contenant 'projet' ou 'Nom du projet'.
    Retourne la ligne nettoyÃ©e.
    """
    lines = cahier.split("\n")
    for line in lines:
        line_clean = line.strip().replace("\u200b", "")  # supprime les caractÃ¨res invisibles
        # Cherche "projet" ou "nom du projet" (case insensitive)
        if re.search(r"\bprojet\b", line_clean, re.IGNORECASE):
            return line_clean
    return "Titre du projet non trouvÃ©"

import requests

def create_github_scrum_board(
    token: str,
    username: str,
    project_title: str,
    sprint_backlogs: list,
    estimated_backlog: list,
    assignments: list = [],
     team_members: list = []
) -> dict:
    """
    CrÃ©e automatiquement :
    - Un repo GitHub nommÃ© d'aprÃ¨s le titre du projet
    - Un Project Board Scrum avec les user stories assignÃ©es et colorÃ©es par sprint
    """

    headers_rest = {
        "Accept": "application/vnd.github+json",
        "Authorization": f"Bearer {token}"
    }
    headers_graphql = {"Authorization": f"Bearer {token}"}

    # Nettoyer le nom du repo
    repo_name = project_title.strip().lower()
    repo_name = re.sub(r'[^a-z0-9\-]', '-', repo_name)
    repo_name = re.sub(r'-+', '-', repo_name).strip('-')

    # â”€â”€ 1. CrÃ©er ou rÃ©cupÃ©rer le repo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    repo_check = requests.get(
        f"https://api.github.com/repos/{username}/{repo_name}",
        headers=headers_rest
    )

    if repo_check.status_code == 200:
        repo_info = repo_check.json()
        print(f"Repo '{repo_name}' existe dÃ©jÃ  : {repo_info['html_url']}")
    else:
        repo_resp = requests.post(
            "https://api.github.com/user/repos",
            json={
                "name": repo_name,
                "description": f"Scrum planning pour : {project_title}",
                "private": False
            },
            headers=headers_rest
        )
        if repo_resp.status_code != 201:
            raise Exception(f"Erreur crÃ©ation repo : {repo_resp.json()}")
        repo_info = repo_resp.json()
        print(f"Repo crÃ©Ã© : {repo_info['html_url']}")

    repo_name_clean = repo_info["name"]
    repo_url = repo_info["html_url"]

    # â”€â”€ 2. CrÃ©er les labels colorÃ©s par sprint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Couleurs distinctes pour chaque sprint
    sprint_colors = [
        "0075ca",  # Sprint 1 - bleu
        "e4e669",  # Sprint 2 - jaune
        "d93f0b",  # Sprint 3 - rouge
        "0e8a16",  # Sprint 4 - vert
        "5319e7",  # Sprint 5 - violet
        "f9d0c4",  # Sprint 6 - rose
        "1d76db",  # Sprint 7 - bleu clair
        "b60205",  # Sprint 8 - rouge foncÃ©
    ]

    sprint_label_map = {}  # sprint_num â†’ label name

    for sprint in sprint_backlogs:
        sprint_num = sprint["sprint"]
        color = sprint_colors[(sprint_num - 1) % len(sprint_colors)]
        label_name = f"Sprint {sprint_num}"

        # CrÃ©er le label (ignorer si dÃ©jÃ  existant)
        label_resp = requests.post(
            f"https://api.github.com/repos/{username}/{repo_name_clean}/labels",
            json={
                "name": label_name,
                "color": color,
                "description": f"Issues du Sprint {sprint_num}"
            },
            headers=headers_rest
        )

        if label_resp.status_code in [201, 422]:  # 422 = dÃ©jÃ  existant
            sprint_label_map[sprint_num] = label_name
            print(f"ğŸ·ï¸  Label '{label_name}' (#{color}) prÃªt")
        else:
            print(f"âš ï¸ Erreur label Sprint {sprint_num}: {label_resp.json()}")

    # â”€â”€ 3. RÃ©cupÃ©rer les usernames GitHub des membres â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Construire mapping : nom membre â†’ github username
    # On essaie de rÃ©soudre via l'API GitHub search
    members_github = {}  # nom â†’ github_login

    all_member_names = set()
    for assignment in assignments:
        assigned_to = assignment.get("assigned_to")
        if assigned_to:
            all_member_names.add(assigned_to)

    for member_name in all_member_names:
        # Chercher le user GitHub par son nom
        search_resp = requests.get(
            f"https://api.github.com/search/users?q={member_name}+in:login",
            headers=headers_rest
        )
        if search_resp.status_code == 200:
            items = search_resp.json().get("items", [])
            if items:
                members_github[member_name] = items[0]["login"]
                print(f"ğŸ‘¤ Membre trouvÃ© : {member_name} â†’ @{items[0]['login']}")
            else:
                # fallback : utiliser le username du token
                members_github[member_name] = username
                print(f"âš ï¸ Membre '{member_name}' non trouvÃ© sur GitHub â†’ assignÃ© Ã  @{username}")
        else:
            members_github[member_name] = username

    # â”€â”€ 4. Construire les mappings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    stories_map = {s["id"]: s for s in estimated_backlog}
    assignments_map = {a["story_id"]: a for a in assignments}  # story_id â†’ assignment

    # â”€â”€ 5. RÃ©cupÃ©rer owner ID et crÃ©er le board â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    owner_resp = requests.post(
        "https://api.github.com/graphql",
        json={"query": f'{{ user(login:"{username}") {{ id }} }}'},
        headers=headers_graphql
    ).json()
    owner_id = owner_resp["data"]["user"]["id"]

    project_resp = requests.post(
        "https://api.github.com/graphql",
        json={
            "query": f"""
            mutation {{
              createProjectV2(input: {{
                ownerId: "{owner_id}",
                title: "{project_title}"
              }}) {{
                projectV2 {{ id url }}
              }}
            }}
            """
        },
        headers=headers_graphql
    ).json()

    if project_resp.get("errors"):
        latest = requests.post(
            "https://api.github.com/graphql",
            json={
                "query": f"""
                {{
                  user(login: "{username}") {{
                    projectsV2(first:5, orderBy: {{field:CREATED_AT, direction:DESC}}) {{
                      nodes {{ id title url }}
                    }}
                  }}
                }}
                """
            },
            headers=headers_graphql
        ).json()["data"]["user"]["projectsV2"]["nodes"][0]
        project_id = latest["id"]
        project_url = latest["url"]
        print(f"Board existant utilisÃ© : {project_url}")
    else:
        project_id = project_resp["data"]["createProjectV2"]["projectV2"]["id"]
        project_url = project_resp["data"]["createProjectV2"]["projectV2"]["url"]
        print(f"Board crÃ©Ã© : {project_url}")

    # â”€â”€ 6. CrÃ©er les issues avec assignees + labels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    created_issues = []
    errors_issues = []

    for sprint in sprint_backlogs:
        sprint_num = sprint["sprint"]
        print(f"\nğŸ“‹ Sprint {sprint_num} â€” {len(sprint['items'])} stories")

        for story_id in sprint["items"]:
            story = stories_map.get(story_id)
            assignment = assignments_map.get(story_id)

            if story:
                title = story.get("title", story_id)
                points = story.get("points", "?")
                risk = story.get("risk", "")
                complexity = story.get("complexity", "")
            else:
                print(f"âš ï¸ Story {story_id} non trouvÃ©e")
                title = story_id
                points = "?"
                risk = "unknown"
                complexity = "unknown"

            # RÃ©cupÃ©rer l'assignee GitHub
            assigned_to_name = assignment.get("assigned_to") if assignment else None
            github_login = members_github.get(assigned_to_name, username) if assigned_to_name else username
            assignees = [github_login]

            issue_title = f"[Sprint {sprint_num}] {title}"
            issue_body = (
                f"## ğŸ“Œ {title}\n\n"
                f"| Champ | Valeur |\n"
                f"|-------|--------|\n"
                f"| **Story ID** | {story_id} |\n"
                f"| **Sprint** | {sprint_num} |\n"
                f"| **Story Points** | {points} |\n"
                f"| **Risk** | {risk} |\n"
                f"| **Complexity** | {complexity} |\n"
                f"| **AssignÃ© Ã ** | @{github_login} |\n"
            )

            # CrÃ©er l'issue REST avec assignee + label
            issue_resp = requests.post(
                f"https://api.github.com/repos/{username}/{repo_name_clean}/issues",
                json={
                    "title": issue_title,
                    "body": issue_body,
                    "assignees": assignees,
                    "labels": [sprint_label_map.get(sprint_num, f"Sprint {sprint_num}")]
                },
                headers=headers_rest
            )

            if issue_resp.status_code != 201:
                print(f"âŒ Erreur issue '{issue_title}': {issue_resp.status_code} â†’ {issue_resp.json()}")
                errors_issues.append({
                    "story_id": story_id,
                    "error": issue_resp.json()
                })
                continue

            issue_data = issue_resp.json()
            issue_node_id = issue_data["node_id"]
            issue_url = issue_data["html_url"]

            # Ajouter au Project Board
            add_resp = requests.post(
                "https://api.github.com/graphql",
                json={
                    "query": f"""
                    mutation {{
                      addProjectV2ItemById(input:{{
                        projectId:"{project_id}",
                        contentId:"{issue_node_id}"
                      }}) {{
                        item {{ id }}
                      }}
                    }}
                    """
                },
                headers=headers_graphql
            ).json()

            if add_resp.get("errors"):
                print(f"âŒ Erreur ajout board '{issue_title}': {add_resp['errors']}")
                errors_issues.append({
                    "story_id": story_id,
                    "error": str(add_resp["errors"])
                })
            else:
                created_issues.append({
                    "story_id": story_id,
                    "sprint": sprint_num,
                    "title": issue_title,
                    "url": issue_url,
                    "assignee": github_login
                })
                print(f"âœ… {story_id} â†’ @{github_login} | {issue_url}")

    print(f"\nğŸ“Š RÃ©sumÃ© final : {len(created_issues)} crÃ©Ã©es, {len(errors_issues)} erreurs")

    return {
        "ok": True,
        "repo_url": repo_url,
        "board_url": project_url,
        "issues_created": len(created_issues),
        "issues": created_issues,
        "errors": errors_issues
    }

if __name__ == "__main__":
    
    graph = build_scrum_graph()
    cahier_de_charge = """
Cahier des Charges â€” Ã‰quipe Joy
Projet SummerCamp 2025
PorterVision â€” Plateforme dâ€™analyse stratÃ©gique automatisÃ©e

1. **Contexte**
PorterVision est une plateforme moderne qui aide les entreprises Ã  analyser leurs projets avec une mÃ©thode stratÃ©gique.
Le but est dâ€™amÃ©liorer la prise de dÃ©cision grÃ¢ce Ã  lâ€™intelligence artificielle.

2. **Objectifs**
- RÃ©duire le temps de prise de dÃ©cision de 30% grÃ¢ce Ã  l'analyse automatique des donnÃ©es.
- GÃ©nÃ©rer des recommandations prÃ©cises et personnalisÃ©es avec une prÃ©cision de 90% (mesurÃ©e via des retours utilisateurs).
- Fournir un dashboard clair et facile Ã  utiliser, avec une interface rÃ©pondant aux critÃ¨res de Nielsen pour lâ€™ergonomie.

3. **PÃ©rimÃ¨tre**
Les limites de projet doivent Ãªtre dÃ©finies pour garantir que la plateforme PorterVision ne dÃ©passe pas les ressources disponibles.
- **Ressources humaines** : Ã‰quipe de 5 Ã  7 personnes (dÃ©veloppement, IA, design).
- **Ressources matÃ©rielles** : Infrastructure cloud AWS/GCP avec un budget maximal de 500 000 â‚¬.
- **Contraintes techniques** : Utilisation exclusive des stacks dÃ©finis dans les contraintes techniques.

4. **Acteurs**
- Admin
- Utilisateur

5. **FonctionnalitÃ©s attendues**
- Upload de fichiers (formats supportÃ©s : CSV, Excel, PDF).
- Analyse automatique (traitement en temps rÃ©el des donnÃ©es).
- Dashboard (visualisation en temps rÃ©el avec filtres dynamiques).
- GÃ©nÃ©ration de rapport (export PDF/Excel).
- Recommandations IA (basÃ©es sur des modÃ¨les prÃ©-entraÃ®nÃ©s).

6. **Contraintes techniques**
- Backend : Python ou Java ou NodeJS
- Frontend : React ou Angular ou Vue
- Base de donnÃ©es : PostgreSQL ou MongoDB
- Agents IA : LangChain ou CrewAI ou AutoGen

7. **Exigences non fonctionnelles**
- DisponibilitÃ© de 99,9% sur 24/7 avec un temps de rÃ©ponse infÃ©rieur Ã  2 secondes.
- SÃ©curitÃ© : ConformitÃ© RGPD, chiffrement des donnÃ©es en transit et au repos.
- ScalabilitÃ© : Support de 10 000 utilisateurs simultanÃ©s.

8. **KPI**
- PrÃ©cision des recommandations IA : 90% (mesurÃ©e via des tests A/B).
- RÃ©duction du temps de prise de dÃ©cision de 30% (mesurÃ©e via des Ã©tudes de cas).

9. **Livrables**
- Application web (responsive, multiplateforme).
- API RESTful documentÃ©e.
- Rapport PDF (gÃ©nÃ©rÃ© automatiquement avec les analyses).

10. **Ã‰quipe projet**
Lâ€™Ã©quipe sera dÃ©finie plus tard.
"""
   

    team = {
        "sprint_length_days": 14,
        "sprint_capacity_points": 20,
        "members": [
            {"name": "Sami", "role": "Backend", "skills": ["backend", "sql", "spring"]},
            {"name": "Ali", "role": "Frontend", "skills": ["frontend", "angular", "ui"]},
            {"name": "Mouna", "role": "DevOps", "skills": ["devops", "docker", "ci_cd"]},
            {"name": "Hela", "role": "QA", "skills": ["qa", "testing"]}
        ]
    }

    result = graph.invoke({
        "cahier_de_charge": cahier_de_charge,
        "team": team,
        "validation_attempts": 0,
        "max_validation_attempts": 3
    })

    print(result)
    if(result["spec_validation"]["ok"]==False):
        print("validation failed")
        print(result["spec_validation"]["errors"])
        print(result["spec_fix_suggestions"])
    else:
        print("============================================")
        print("Requirements:", result["requirements"])
        print("============================================")
        print("Product Backlog:", result["product_backlog"])
        print("============================================")
        print("Estimated Backlog:", result["estimated_backlog"])
        print("============================================")
        print("Dependencies:", result.get("dependencies", []))
        print("============================================")
        print("Sprints:", result["sprint_backlogs"])
        print("============================================")
        print("Assignments:", result["assignments"])
        print("============================================")
        print("Validation:", result["validation"])
        print("============================================")

# --- Build chosen_fixes from the suggestion generator output ---
    chosen_fixes = [
        {
            "error_code": "REQUIREMENTS_TOO_HIGH_LEVEL",
            "severity": "medium",
            "message": "Les objectifs sont trop gÃ©nÃ©raux et nÃ©cessitent une clarification",
            "evidence": "Les objectifs sont trop gÃ©nÃ©raux et nÃ©cessitent une clarification",
            "fix_id": "S1",
            "title": "DÃ©finir des objectifs plus spÃ©cifiques et mesurables",
            "paragraph": "Les objectifs doivent Ãªtre dÃ©finis de maniÃ¨re plus spÃ©cifique et mesurable pour garantir que la plateforme PorterVision rÃ©pond aux besoins rÃ©els des utilisateurs. Par exemple, l'objectif pourrait Ãªtre de 'rÃ©duire le temps de prise de dÃ©cision de 30% grÃ¢ce Ã  l'analyse automatique des donnÃ©es'."
        },
        {
            "error_code": "NFR_TOO_WEAK",
            "severity": "high",
            "message": "Les exigences non-fonctionnelles sont insuffisantes et nÃ©cessitent une complÃ©tion",
            "evidence": "Les exigences non-fonctionnelles sont insuffisantes et nÃ©cessitent une complÃ©tion",
            "fix_id": "S1",
            "title": "DÃ©finir des exigences non-fonctionnelles plus dÃ©taillÃ©es",
            "paragraph": "Les exigences non-fonctionnelles doivent Ãªtre dÃ©finies de maniÃ¨re plus dÃ©taillÃ©e pour garantir que la plateforme PorterVision rÃ©pond aux besoins rÃ©els des utilisateurs. Par exemple, l'exigence non-fonctionnelle pourrait Ãªtre 'la plateforme doit Ãªtre accessible en ligne 24/7 avec une disponibilitÃ© de 99,9%'."
        },
        {
            "error_code": "OTHER",
            "severity": "medium",
            "message": "Le pÃ©rimÃ¨tre du projet est trop vague et nÃ©cessite une clarification",
            "evidence": "Le pÃ©rimÃ¨tre du projet est trop vague et nÃ©cessite une clarification",
            "fix_id": "S2",
            "title": "DÃ©finir des limites de projet",
            "paragraph": "Les limites de projet doivent Ãªtre dÃ©finies pour garantir que la plateforme PorterVision ne dÃ©passe pas les ressources disponibles. Par exemple, les limites de projet pourraient inclure la dÃ©finition des ressources humaines, des ressources matÃ©rielles, des budgets, etc."
        }
    ]


    result=merge_app.invoke({ "spec_original": cahier_de_charge, "chosen_fixes": chosen_fixes, "manual_patch": "" })
    print("Enhanced spec after merging chosen fixes:"+result["spec_enhanced"])

