# -*- coding: utf-8 -*-
"""main 7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11DBK7gkPGcnGCn-YtNtEs4ikp_lImfYY
"""



# 2️⃣ Importer les modules
from __future__ import annotations
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import pytesseract
import pytesseract
pytesseract.pytesseract.tesseract_cmd = "C:/Program Files/Tesseract-OCR/tesseract.exe"

from typing import TypedDict,List

from langgraph.graph import StateGraph, END
from langchain_groq import ChatGroq
from langgraph.prebuilt import ToolNode

from langchain_core.messages import HumanMessage, AIMessage, BaseMessage,ToolMessage

import json
import ast
from typing import Any


import io
from typing import Union
import pdfplumber
from docx import Document
from typing import TypedDict, List, Dict, Any, Optional
from dataclasses import dataclass
import math

from langgraph.graph import StateGraph, END

class ScrumState(TypedDict, total=False):

    cahier_de_charge: str
    team: Dict[str, Any]
    spec_validation_step:bool
    validation_attempts: int
    max_validation_attempts: int

    cleaned_spec: str
    spec_cleaning: Dict[str, Any]      # info about cleaning steps
    spec_validation: Dict[str, Any]    # ok/errors

    spec_fix_suggestions: dict | None
    spec_fix_mode: str | None   # "waiting_user" | "auto_apply"
    enhanced_spec: str | None

    requirements: List[Dict[str, Any]]
    product_backlog: List[Dict[str, Any]]
    refined_backlog: List[Dict[str, Any]]
    estimated_backlog: List[Dict[str, Any]]
    dependencies: List[Dict[str, Any]]

    sprint_backlogs: List[Dict[str, Any]]
    assignments: List[Dict[str, Any]]

    validation: Dict[str, Any]

class ChosenFix(TypedDict):
    error_code: str
    severity: str
    message: str
    evidence: str
    fix_id: str
    title: str
    paragraph: str



class SpecMergeState(TypedDict, total=False):
    spec_original: str
    chosen_fixes: List[ChosenFix]
    manual_patch: str
    spec_enhanced: str
    merge_report: Dict[str, Any]

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

def describe_image(image: Image.Image) -> str:
    # OCR first
    text_in_image = pytesseract.image_to_string(image).strip()

    if text_in_image:
        return f"[IMAGE OCR TEXT]\n{text_in_image}"
    else:
        inputs = processor(images=image, return_tensors="pt")
        out = model.generate(**inputs, max_length=150)
        caption = processor.decode(out[0], skip_special_tokens=True)
        return f"[IMAGE CAPTION]\n{caption}"

def extract_txt_from_bytes(file_bytes: bytes) -> str:
    return file_bytes.decode("utf-8", errors="ignore")


def extract_docx_from_bytes(file_bytes: bytes) -> str:
    doc = Document(io.BytesIO(file_bytes))

    parts = []
    parts.append("===== DOCX TEXT =====\n")

    for p in doc.paragraphs:
        t = p.text.strip()
        if t:
            parts.append(t)

    parts.append("\n===== DOCX IMAGES =====\n")

    rels = doc.part._rels
    img_count = 0

    for rel in rels.values():
        if "image" in rel.target_ref:
            img_count += 1
            img_data = rel.target_part.blob
            image = Image.open(io.BytesIO(img_data)).convert("RGB")
            parts.append(f"\n--- Image {img_count} ---\n{describe_image(image)}\n")

    return "\n".join(parts)


def extract_pdf_from_bytes(file_bytes: bytes) -> str:
    parts = []
    parts.append("===== PDF TEXT =====\n")

    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        img_count = 0

        for page_index, page in enumerate(pdf.pages, start=1):
            page_text = (page.extract_text() or "").strip()
            if page_text:
                parts.append(f"\n--- Page {page_index} ---\n{page_text}\n")

            if page.images:
                parts.append(f"\n--- Page {page_index} IMAGES ---\n")

            for img in page.images:
                try:
                    bbox = (img["x0"], img["top"], img["x1"], img["bottom"])
                    cropped = page.crop(bbox).to_image(resolution=200).original
                    pil_img = cropped.convert("RGB")

                    img_count += 1
                    parts.append(
                        f"\n[Image {img_count} - Page {page_index}]\n{describe_image(pil_img)}\n"
                    )

                except Exception as e:
                    parts.append(f"\n[Image extraction failed - Page {page_index}] {str(e)}\n")

    return "\n".join(parts)


def extract_document_with_images(
    file_obj: Union[bytes, "UploadFile", io.BytesIO],
    filename: str
) -> str:
    """
    Accepts a REAL file object:
    - bytes
    - BytesIO
    - FastAPI UploadFile
    Needs `filename` to detect extension.
    Returns one combined string.
    """

    ext = filename.lower().split(".")[-1]

    # Convert input to bytes
    if isinstance(file_obj, bytes):
        file_bytes = file_obj

    elif isinstance(file_obj, io.BytesIO):
        file_bytes = file_obj.getvalue()

    else:
        # FastAPI UploadFile
        # (UploadFile.file is a SpooledTemporaryFile)
        file_bytes = file_obj.file.read()

    # Dispatch by extension
    if ext == "pdf":
        return extract_pdf_from_bytes(file_bytes)

    elif ext == "docx":
        return extract_docx_from_bytes(file_bytes)

    elif ext == "txt":
        return extract_txt_from_bytes(file_bytes)

    else:
        raise ValueError("Unsupported file type. Only PDF, DOCX, TXT.")

import json
from typing import Dict, Any
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage
model="llama-3.1-8b-instant"
llm = ChatGroq(
    model=model,
    temperature=0
)

def llm_json(prompt: str) -> Dict[str, Any]:
    response = llm.invoke([
        HumanMessage(content=prompt + "\n\nReturn ONLY valid JSON. No markdown.")
    ])

    text = response.content.strip()


    # Optional: handle ```json blocks
    print("old text is:"+text)
    if text.find("```json")!=-1:
        start_index = text.find("```json")
        end_index = text.find("```", start_index + 7)
        text = text[start_index + 7:end_index]
        print("new text is:"+text)
    elif text.find("```")!=-1:
        start_index = text.find("```")
        end_index = text.find("```", start_index + 3)
        text = text[start_index + 3:end_index]
        print("new text is:"+text)

    return parse_llm_json_or_python_dict(text)
def llm_text(prompt: str) -> str:
    """
    Returns plain text from the LLM.
    No JSON parsing.
    """
    resp = llm.invoke(prompt)
    return resp.content.strip()

import json, ast, re

def parse_llm_json_or_python_dict(text: str):
    text = text.strip()

    # 1) try JSON first
    try:
        return json.loads(text)
    except Exception:
        pass

    # 2) try python dict safely
    try:
        return ast.literal_eval(text)
    except Exception:
        pass

    m = re.search(r"\{.*\}", text, re.DOTALL)
    if m:
        candidate = m.group(0)
        try:
            return json.loads(candidate)
        except Exception:
            try:
                return ast.literal_eval(candidate)
            except Exception:
                pass

    return None

def clean_specifications_llm_node(state: ScrumState) -> ScrumState:

    raw_spec = (state.get("cahier_de_charge") or "").strip()

    prompt = f"""
You are a senior business analyst.

Task:
Rewrite the following specification into a clean, structured version.

Rules:
- Keep the meaning the same
- Remove duplication
- Fix unclear sentences
- Output MUST be plain text (not JSON)
- Use this structure:

TITLE:
SCOPE:
ACTORS:
FUNCTIONAL REQUIREMENTS:
NON-FUNCTIONAL REQUIREMENTS:
CONSTRAINTS:
OUT OF SCOPE:
OPEN QUESTIONS:

Specification:
{raw_spec}
""".strip()

    resp = llm.invoke([HumanMessage(content=prompt)])

    state["cleaned_spec"] = resp.content.strip()
    state["spec_cleaning"] = {
        "ok": True,
        "method": "llm_rewrite",
        "model": model
    }
    print("cleaned_spec is:"+state["cleaned_spec"])
    return state

def route_after_clean_specifications(state: ScrumState) -> str:
    return "DoNotSkipValidation" if state["spec_validation_step"]==True else "SkipValidation"
def validate_specifications_llm_node(state: ScrumState) -> ScrumState:

    spec = state.get("cahier_de_charge")

    if not spec:
        state["spec_validation"] = {
            "ok": False,
            "errors": [{
                "code": "OTHER",
                "message": "Le cahier des charges est vide.",
                "severity": "high",
                "hint": "Veuillez fournir un cahier des charges."
            }]
        }
        return state

    prompt = f"""
Tu es un auditeur de cahier des charges.

TÂCHE:
Faire une vérification LÉGÈRE de la qualité du cahier des charges.

CONSIGNES:
- Ne sois PAS trop strict
- Vérifie juste les problèmes ÉVIDENTS et IMPORTANTS
- Si le cahier des charges est globalement acceptable => ok = true
- Ne cherche pas la perfection

TYPES DE PROBLÈMES À DÉTECTER (seulement si VRAIMENT problématiques):
- SCOPE_TOO_VAGUE: Le scope est vraiment trop flou pour être implémenté
- REQUIREMENTS_TOO_HIGH_LEVEL: Les exigences sont beaucoup trop générales
- NFR_TOO_WEAK: Les exigences non-fonctionnelles sont vraiment manquantes
- AMBIGUOUS: Il y a des contradictions claires
- INCONSISTENT: Il y a des incohérences importantes
- OTHER: Autre problème majeur

RÈGLES DE SORTIE:
- Si errors contient au moins 1 élément => ok = false
- Si ok = true => errors = []
- Le champ "message" ne doit JAMAIS être vide
- Renvoie uniquement du JSON valide

SCHEMA JSON:
{{
  "ok": true ou false,
  "errors": [
    {{
      "code": "SCOPE_TOO_VAGUE|REQUIREMENTS_TOO_HIGH_LEVEL|NFR_TOO_WEAK|AMBIGUOUS|INCONSISTENT|OTHER",
      "message": "Description du problème",
      "severity": "low|medium|high",
      "hint": "Conseil pour améliorer"
    }}
  ]
}}

CAHIER DES CHARGES:
{spec}

Réponds UNIQUEMENT avec du JSON valide.
""".strip()

    resp = llm_json(prompt)

    # Light validation: just check structure
    errors = resp.get("errors", [])
    filtered = []

    for e in errors:
        if not isinstance(e, dict):
            continue

        message = (e.get("message") or "").strip()

        # Only requirement: message must not be empty
        if message:
            # Remove evidence field if present
            if "evidence" in e:
                del e["evidence"]
            filtered.append(e)

    resp["errors"] = filtered
    resp["ok"] = (len(filtered) == 0)

    state["spec_validation"] = resp
    return state

def generate_spec_fix_suggestions_node(state: ScrumState) -> ScrumState:
    """
    Génère <= 3 suggestions de correction par erreur de validation.
    Chaque suggestion est un petit paragraphe prêt à afficher dans le frontend.
    """

    spec = (state.get("cleaned_spec") or state.get("cahier_de_charge") or "").strip()
    validation = state.get("spec_validation") or {}
    errors = validation.get("errors", [])

    # Si aucune erreur, rien à faire
    if not errors:
        state["spec_fix_suggestions"] = {"ok": True, "suggestions": []}
        return state

    prompt = f"""
Tu es un auditeur de cahier des charges.

TÂCHE:
Étant donné un cahier des charges (en français) et la liste des erreurs détectées,
les reponses generé doit etre en français
génère des suggestions concrètes pour corriger ces erreurs.

RÈGLES:
- Pour CHAQUE erreur, retourne AU MAXIMUM 3 suggestions.
- Chaque suggestion doit être un petit paragraphe (2 à 5 lignes).
- Les suggestions doivent être concrètes et directement insérables dans le cahier des charges.
- NE PAS inventer de nouvelles fonctionnalités.
- NE PAS réécrire tout le document.
- Si l'erreur est de sévérité faible, fais des suggestions courtes.
- Retourne UNIQUEMENT du JSON valide (guillemets doubles uniquement).

SCHEMA DE SORTIE:
{{
  "ok": true,
  "suggestions": [
    {{
      "error_code": "string",
      "severity": "low|medium|high",
      "message": "string",
      "evidence": "string",
      "fixes": [
        {{
          "id": "S1",
          "title": "string",
          "paragraph": "string"
        }}
      ]
    }}
  ]
}}

ERREURS DE VALIDATION:
{errors}

CAHIER DES CHARGES:
{spec}
""".strip()

    resp = llm_json(prompt)
    state["spec_fix_suggestions"] = resp

    # Le pipeline doit s'arrêter ici pour attendre le choix utilisateur côté frontend
    state["spec_fix_mode"] = "waiting_user"
    return state

def route_after_spec_validation(state: ScrumState) -> str:
    return "valid" if state["spec_validation"].get("ok") else "invalid"

def extract_requirements_node(state: ScrumState) -> ScrumState:
    spec = state.get("cleaned_spec", state["cahier_de_charge"])


    prompt = f"""
Extract requirements from this cahier de charge.

Return JSON with:
requirements: [
  {{
    "id": "R1",
    "type": "functional|nfr",
    "text": "...",
    "priority": "must|should|could",
    "notes": "optional"
  }}
]

SPEC:
{spec}
"""
    out = llm_json(prompt)
    print("requirement out is:"+str(out))
    state["requirements"] = out
    return state


def generate_product_backlog_node(state: ScrumState) -> ScrumState:
    reqs = state["requirements"]
    #we will just need the functional requirements for the backlog
    filtered_reqs=[]
    print("reqs is:"+str(reqs))
    print("length:"+str(len(reqs)))
    if(type(reqs) != list):
        for req in reqs["requirements"]:
            print("1 requirement is:"+str(req))
            if req.get("type").find("functional")!=-1:
                filtered_reqs.append(req)
            print("total requirements:"+str(len(reqs["requirements"])))
            print("total filtered requirements:"+str(len(filtered_reqs)))
    else:
        for req in reqs:
            print("1 requirement is:"+str(req))
            if req.get("type").find("functional")!=-1:
                filtered_reqs.append(req)
            print("total requirements:"+str(len(reqs)))
            print("total filtered requirements:"+str(len(filtered_reqs)))

    prompt = f"""
    Tu es un Scrum Master senior.

    TÂCHE:
    Convertir les exigences en Product Backlog Scrum.

    RÈGLES IMPORTANTES:
    - required_skills doit contenir UNIQUEMENT les compétences réellement nécessaires pour cette User Story.
    - Ne mets PAS automatiquement ["backend","frontend","devops","qa"].
    - Utilise seulement les valeurs suivantes:
      ["backend","frontend","devops","qa","data","ml","security","ux_ui"]
    - Minimum 1 skill, maximum 3 skills.
    - Si une story est purement UI => frontend ou ux_ui.
    - Si une story est purement API/DB => backend ou data.
    - Si une story est sur déploiement/CI/CD => devops.
    - Si une story est sur tests => qa.
    - Si une story est sur NLP/LLM => ml.
    - Si une story est sur auth/chiffrement => security.

    FORMAT:
    - Retourne UNIQUEMENT un JSON valide.
    - Utilise uniquement des guillemets doubles " ".
    - Aucun texte avant/après.

    SCHEMA:
    {{
      "product_backlog": [
        {{
          "epic": "string",
          "stories": [
            {{
              "id": "US1",
              "title": "string",
              "as_a": "string",
              "i_want": "string",
              "so_that": "string",
              "acceptance_criteria": ["string"],
              "required_skills": ["string"],
              "priority": "must|should|could",
            }}
          ]
        }}
      ]
    }}

    REQUIREMENTS:
    {filtered_reqs}
    """.strip()

    out = llm_json(prompt)

    #print("productBacklog out is:"+str(out))
    state["product_backlog"] = out["product_backlog"]
    return state


def refine_backlog_node(state: ScrumState) -> ScrumState:
    pb = state["product_backlog"]

    if(state.get("validation", {}).get("ok", False) == False):
        validation_feedback = state.get("validation")
    else:
        validation_feedback = {"ok":True}
    print("validation_feedback is:"+str(validation_feedback))
    prompt = f"""
Refine the backlog using INVEST:
- split stories that are too big
- remove duplicates
- add missing acceptance criteria
Return JSON with:
refined_backlog: [
  {{
    "id": "US1",
    "title": "...",
    "description": "...",
    "acceptance_criteria": ["..."],
    "required_skills": ["backend","frontend","devops","qa"],
    "priority": "must|should|could",
  }}
]


IMPORTANT:
- Output ONLY JSON.
- No python code.
- No markdown.
- No explanations.
Validation_feedback:
{validation_feedback}
PRODUCT_BACKLOG:
{pb}
"""
    out = llm_json(prompt)
    print("refined_backlog out is:"+str(out))
    state["refined_backlog"] = out
    return state


def estimate_backlog_node(state: ScrumState) -> ScrumState:
    refined = state["refined_backlog"]
    team = state["team"]

    if(state.get("validation", {}).get("ok", False) == False):
        validation_feedback = state.get("validation")
    else:
        validation_feedback = {"ok":True}
    print("validation_feedback is:"+str(validation_feedback))


    prompt = f"""
Estimate each story using Fibonacci story points: 1,2,3,5,8,13,21.

Return JSON with:
{{
  "estimated_backlog": [
    {{
      "id": "...",
      "title": "...",
      "points": 1|2|3|5|8|13|21,
      "risk": "low|medium|high",
      "complexity": "low|medium|high",
      "time_estimation": "1|2|3|5|8|13|21 hours",
      "priority": "must|should|could",


      "required_skills": [...]
    }}
  ]
}}

IMPORTANT:
- Output ONLY JSON.
- No python code.
- No markdown.
- No explanations.

validationFeedback:
{validation_feedback}

TEAM:
{team}

STORIES:
{refined}
"""

    out = llm_json(prompt)
    print("estimated_backlog out is:"+str(out))
    state["estimated_backlog"] = out["estimated_backlog"]
    return state


def map_dependencies_node(state: ScrumState) -> ScrumState:
    stories = state["estimated_backlog"]

    prompt = f"""
Detect dependencies between stories.

Return JSON with:
dependencies: [
  {{
    "from": "US1",
    "to": "US5",
    "type": "blocks"
  }}
]
STORIES:
{stories}
"""
    out = llm_json(prompt)
    print("dependencies out is:"+str(out))
    try:
        state["dependencies"] = out["dependencies"]
    except:
        state["dependencies"] = out
    return state

def sprint_planner_node(state: ScrumState) -> ScrumState:
    stories = state["estimated_backlog"]
    deps = state.get("dependencies", [])
    capacity = int(state["team"].get("sprint_capacity_points", 20))

    # Simple heuristic: order by dependencies first (very simplified)
    # (In production: topological sort)
    ordered = stories[:]  # assume already prioritized by LLM

    sprints = []
    current = {"sprint": 1, "items": [], "total_points": 0}

    for st in ordered:
        pts = int(st["points"])
        if current["total_points"] + pts > capacity and current["items"]:
            sprints.append(current)
            current = {"sprint": current["sprint"] + 1, "items": [], "total_points": 0}

        current["items"].append(st["id"])
        current["total_points"] += pts

    if current["items"]:
        sprints.append(current)

    state["sprint_backlogs"] = sprints
    return state

def contributor_assigner_node(state: ScrumState) -> ScrumState:
    team_members = state["team"]["members"]
    stories = {s["id"]: s for s in state["estimated_backlog"]}

    # Track assigned workload per member
    workload_points = {m["name"]: 0 for m in team_members}

    assignments = []

    for sprint in state["sprint_backlogs"]:
        for story_id in sprint["items"]:
            story = stories[story_id]
            req_skills = set(story.get("required_skills", []))
            story_points = int(story.get("points", 0))

            best = None
            best_score = -10**9

            for m in team_members:
                member_skills = set(m.get("skills", []))
                skill_match = len(req_skills.intersection(member_skills))

                # workload penalty
                load = workload_points[m["name"]]

                # scoring function
                score = (skill_match * 10) - load

                if score > best_score:
                    best_score = score
                    best = m

            # assign
            assigned_name = best["name"] if best else None

            if assigned_name:
                workload_points[assigned_name] += story_points

            assignments.append({
                "story_id": story_id,
                "title": story.get("title", ""),
                "assigned_to": assigned_name,
                "reason": f"skill_match={len(req_skills.intersection(set(best.get('skills', []))))}, load_after={workload_points[assigned_name]}"
            })

    state["assignments"] = assignments
    state["workload_points"] = workload_points  # optional debug
    return state



def validation_node(state: ScrumState) -> ScrumState:
    state["validation_attempts"] = int(state.get("validation_attempts", 0)) + 1
    capacity = int(state["team"].get("sprint_capacity_points", 20))
    sprints = state["sprint_backlogs"]
    stories = {s["id"]: s for s in state["estimated_backlog"]}

    issues = []

    for sp in sprints:
        total = sum(int(stories[sid]["points"]) for sid in sp["items"])
        if total > capacity:
            issues.append({
                "type": "over_capacity",
                "sprint": sp["sprint"],
                "total_points": total,
                "capacity": capacity
            })

    # Example: detect too big stories
    for s in state["estimated_backlog"]:
        if int(s["points"]) >= 13:
            issues.append({
                "type": "story_too_big",
                "story_id": s["id"],
                "points": s["points"]
            })

    state["validation"] = {
        "ok": len(issues) == 0,
        "issues": issues
    }
    return state

def route_after_validation(state: ScrumState) -> str:
    attempts = int(state.get("validation_attempts", 0))
    max_attempts = int(state.get("max_validation_attempts", 3))

    if attempts >= max_attempts and not state["validation"]["ok"]:
        state["validation"]["stopped_reason"] = "Max validation attempts reached"

        return "stop"
    if state["validation"]["ok"]:
        print("validation ok, we can continue")
        return "done"
    # if stories too big -> refine again
    for issue in state["validation"]["issues"]:
        if issue["type"] == "story_too_big":
            print("we will refine because of issue:"+str(issue))
            return "refine"
    # if over capacity -> plan again
    print("we will plan again because of over capacity")
    return "replan"

def merge_spec_with_chosen_fixes_node(state: SpecMergeState) -> SpecMergeState:
    spec = (state.get("spec_original") or "").strip()
    chosen = state.get("chosen_fixes", [])
    manual_patch = (state.get("manual_patch") or "").strip()

    if not spec:
        state["spec_enhanced"] = ""
        state["merge_report"] = {"ok": False, "reason": "spec_original is empty"}
        return state

    if not chosen and not manual_patch:
        state["spec_enhanced"] = spec
        state["merge_report"] = {"ok": True, "applied_fixes": 0, "note": "no changes"}
        return state

    prompt = f"""
Tu es un assistant expert en rédaction de cahier des charges.

OBJECTIF:
Produire une version améliorée du cahier des charges en appliquant UNIQUEMENT les corrections choisies.

RÈGLES STRICTES:
1) Appliquer uniquement les paragraphes fournis dans "CORRECTIONS CHOISIES".
2) Ne pas inventer de nouvelles fonctionnalités.
3) Ne pas supprimer des sections existantes.
4) Intégrer le texte manuel utilisateur en priorité si non vide.
5) Retourner uniquement le texte final (pas de JSON, pas de markdown, pas d'explications).

CORRECTIONS CHOISIES:
{chosen}

TEXTE MANUEL UTILISATEUR:
{manual_patch}

CAHIER DES CHARGES ORIGINAL:
{spec}
""".strip()

    enhanced = llm_text(prompt)

    state["spec_enhanced"] = enhanced
    state["merge_report"] = {
        "ok": True,
        "applied_fixes": len(chosen),
        "manual_patch_used": bool(manual_patch),
    }
    return state

def build_scrum_graph():
    g = StateGraph(ScrumState)
    g.add_node("clean_spec", clean_specifications_llm_node)
    g.add_node("validate_spec", validate_specifications_llm_node)
    g.add_node("extract_requirements", extract_requirements_node)
    g.add_node("generate_product_backlog", generate_product_backlog_node)
    g.add_node("refine_backlog", refine_backlog_node)
    g.add_node("estimate_backlog", estimate_backlog_node)
    g.add_node("map_dependencies", map_dependencies_node)
    g.add_node("sprint_planner", sprint_planner_node)
    g.add_node("contributor_assigner", contributor_assigner_node)
    g.add_node("validation", validation_node)
    g.add_node("spec_suggestions", generate_spec_fix_suggestions_node)


    # Edges
    g.set_entry_point("clean_spec")
    g.add_conditional_edges(
        "clean_spec",
        route_after_clean_specifications,
        {
            "DoNotSkipValidation": "validate_spec",
            "SkipValidation": "extract_requirements"
        }
    )

    g.add_conditional_edges(
        "validate_spec",
        route_after_spec_validation,
        {
            "valid": "extract_requirements",
            "invalid": "spec_suggestions"
        }
    )

    g.add_edge("extract_requirements", "generate_product_backlog")
    g.add_edge("generate_product_backlog", "refine_backlog")

    g.add_edge("refine_backlog", "estimate_backlog")
    g.add_edge("estimate_backlog", "map_dependencies")
    g.add_edge("map_dependencies", "sprint_planner")
    g.add_edge("sprint_planner", "contributor_assigner")
    g.add_edge("contributor_assigner", "validation")
    g.add_edge("spec_suggestions", END)

    # Conditional routing after validation
    g.add_conditional_edges(
        "validation",
        route_after_validation,
        {
            "done": END,
            "refine": "estimate_backlog",
            "replan": "sprint_planner",
            "stop": END
        }
    )

    return g.compile()

from langgraph.graph import StateGraph, END

merge_graph = StateGraph(SpecMergeState)
merge_graph.add_node("merge_spec", merge_spec_with_chosen_fixes_node)

merge_graph.set_entry_point("merge_spec")
merge_graph.add_edge("merge_spec", END)

merge_app = merge_graph.compile()

if __name__ == "__main__":
    graph = build_scrum_graph()

    cahier_de_charge = """
Cahier des Charges : équipe Joy
Projet SummerCamp 2025
1. Contexte du projet
- Nom du projet :
PorterVision – Plateforme d’analyse stratégique automatisée basée sur les 5 forces de
Porter.
- Client :
● Direction de la stratégie
● Direction commerciale
● Cellules d’innovation ou transformation digitale
● Bureau d'Études
- Département concerné :
● Direction de la stratégie
● Département marketing et commercial
- Objectifs stratégiques :
● Digitaliser et centraliser les analyses de positionnement stratégique.
● Accélérer la prise de décision commerciale en fournissant des analyses automatisées
et visuelles.
● Intégrer l’IA dans les processus d’amélioration continue et de veille concurrentielle.
● Favoriser une culture data-driven dans les décisions stratégiques.
2. Problématique à résoudre
L’analyse des 5 forces de Porter est un outil stratégique clé pour évaluer la position
concurrentielle d’un projet ou d’une unité business. Cependant, ce processus est aujourd’hui
● manuel et chronophage, nécessitant un travail long d’analyse documentaire,
● non standardisé, avec des résultats variables selon les analystes,
● difficile à exploiter à grande échelle pour plusieurs projets en parallèle.
L’IA peut répondre à cette problématique en automatisant l’extraction, la classification et
l’analyse des informations stratégiques, afin d’apporter une vision rapide, cohérente et
exploitable de l’environnement concurrentiel.
3. Objectifs du projet
● Automatiser l’analyse stratégique des projets à partir de documents textuels grâce
à des techniques d’IA (NLP, classification).
● Prédire l’intensité de chaque force de Porter (forte, modérée, faible) selon les
données extraites.
● Classifier les informations collectées en fonction des 5 forces de Porter
(concurrence, nouveaux entrants, substitution, clients, fournisseurs).
● Générer une cartographie interactive des 5 forces pour chaque projet ou unité
analysée.
● Recommander des actions stratégiques basées sur l’analyse des forces :
différenciation, innovation, positionnement
4. Données disponibles
1. Data Sources:
● Project PDF Files : Descriptions of strategic projects projects(briefs, business plans,
PowerPoint presentations) provided by users. Access: Manual upload by users via
● a secure interface.
● Intelligent Web Scraping: Data extracted automatically from company websites,
LinkedIn, and other platforms. Access: Automated via AI agents with customizable filters.
● Internal Databases (optional): Integration of CRM, ERP, Trello, and internal documents
(Google Drive, SharePoint). Access: Integration via API with user-configurable
settings.
● External Market APIs :Market data (Statista, GDELT), industry trends, economic data
(Google Trends, CB Insights). Access: API key configuration by the administrator.
● Structured User Form Data entered via an intelligent assistant (guided form or interactive
chatbot). Access: Intuitive front-end interface with real-time validation.
DATASET SOURCES (For Agents)
● Crunchbase Startup and competitor information.
● CB Insights Market insights and company analytics.
● World Bank Open Data Economic and development data.
● Google Trends API Search trend data for market analysis.
● Common pretraining context Large-scale web data for AI model training.
● Gartner Reports Industry research and strategic insights.
● Twitter/X API Real-time sentiment and trend data from social media. Internal PDF content,
CRM/ERP logs, user feedback, internal sales data.
2. Data Types
● Free Text Detailed project descriptions, addressed issues, company mission (e.g., "Reduce
downtime by 20%").
● Structured Entities Company name, sector (e.g., manufacturing), competitors, clients,
suppliers.
● Economic Data Market size (in millions ), trends (annual growth), forecasts, entry barriers.
● Qualitative Data Competitive strengths (SWOT analysis), key success factors, user feedback
(e.g., customer satisfaction).
● AI Scores & Analyses Results of Porters Five Forces analysis, Lean recommendations, per
formance projections.
● Conversational Data Chat history with the strategic agent, improvement suggestions (e.g.,
"Optimize storage space").
● Metadata Submission date, user ID, plan version (e.g., v1.2).
Estimated Volume (for MVP Phase)
5.Fonctionnalités attendues
● Analyse des fichiers PDF au niveau automatique : extraction du contexte stratégique
via approche NLP multilingue.
● Générateur de Business Plan : rédaction synthétique en langue naturelle +
exportation au format (PDF).
● Dashboard interactif : visualisation des forces de Porter sous forme de radar,
tableaux, heatmaps.
● Recommandations arborescentes : suggérées par agents IA selon forces en présence.
● Roadmap Produit : préconisation des fonctionnalités à développer avec priorité.
Element Estimated Volume Comment
PDF Files 100-200 documents Provided by internal testers or pilot
partners (e.g., Form Entries
Form Entries 150-300 projects
Manual collection alongside PDF
files (e.g., 200 validated projects).
AI Agent Queries 1,500 executions 35 agents/project OE 300 projects,
managed by local LLM or External API
Saved Versions 900 versions
3 versions/project on average, stored
in IPFS or versioned SQL database.
● Assistant conversationnel : chatbot intelligent intégrant l'ensemble des analyses afin
d’interagir avec l’utilisateur.
● Exportation du rapport imprimable / présentable.
6. Contraintes techniques
- Environnement cible : Web app responsive (desktop/mobile)
- Langages / frameworks privilégiés : Python (FastAPI), Django, React.js, LangChain ou
CrewAI pour les agents, PostgreSQL
- Interopérabilité avec systèmes existants : : Intégration possible avec SharePoint, Google
Drive ou ERP internes (phase 2)
- Sécurité et confidentialité :Accès restreint, chiffrement des données, authentification
firebase authentication
7. Méthodologie proposée
Agile : Kanban
8. Critères de succès / KPI
- Précision / Recall / F-score :
L’équipe souhaite atteindre un F-score supérieur à 0,85. En effet, les modèles utilisés
emploieront un “large” nombre de paramètres et seront entraînés sur des jeux de données
“larges”.
- Réduction de temps / coût :
La réalisation d’un rapport en utilisant la méthodologie des 5 forces de Porter
nécessite de conduire une analyse de marché détaillée. Réunir les ressources et le personnel
nécessaires à cela est un processus pouvant prendre plusieurs semaines, voire plusieurs
mois pour une entreprise de grande envergure. Il en est de même quant à la mobilisation de
personnel au sein de l’entreprise ou au recrutement de sous-traitants (comme une
entreprise spécialisée dans le conseil). La solution proposée doit permettre d’éliminer ces
coûts presque entièrement.
- Satisfaction des utilisateurs finaux :
L’interface doit bénéficier d’une prise en main intuitive et d’une esthétique
minimaliste, permettant aux utilisateurs de focaliser leur attention sur le contenu de leurs
travaux. La transformation d’un ensemble d’idées en un rapport complet doit être un
processus direct, c’est-à-dire sans étapes intermédiaires. L’équipe vise un taux de
satisfaction — l’équivalent de notes issues des opinions d’utilisateurs sur des sources de
téléchargement d’applications ou données à travers des enquêtes d’opinions — d’au moins
90% (en notant que les utilisateurs susmentionnés représentent des professionnels au sein
d’entreprises, le produit étant à visée B2B).
9. Équipe-projet
- Chef de projet : Martin Kirilov-Lilov
- Développeur(s) : Aziz Dridi, Ilef Bennour, Nour Gaboussa, Sarra Mouadeb
- Référent métier : Aya Ben Hmida
10. Livrables attendus
- Rapport de cadrage:
1. Analyse des besoins des différents utilisateurs : entrepreneurs, étudiants, analystes
2. Architecture technique: développement de backend avec Django, et du frontend avec
React.
3. Planning Agile (sprints de 2 semaines, priorisation des features)
- Dataset préparé:
1. 500+ PDFs de business plans ou études de marché labellisés manuellement.
2. Scripts de prétraitement (nettoyage texte, extraction tables/figures).
- Modèle IA entraîné et documenté:
1. Modèle NLP (SpaCy + BERT) pour classifier les 5 forces depuis le texte
2. Module de génération (GPT-like) pour les recommandations stratégiques
3. Fiche technique : précision >85% sur jeu de test, temps d’inférence <5s
- Application / API / Dashboard:
1. Application Web (Frontend)
1.1. Fonctionnalités claires :
1.1.1. Page d’upload (drag & drop de PDF/TXT) avec aperçu du fichier
1.1.2. Bouton "Analyser" déclenchant le traitement IA
1.1.3. Dashboard interactif :
- Visualisation des 5 forces (diagramme radar + scores en %)
-Onglet "Recommandations" (liste priorisée d’améliorations)
-Bouton "Générer le Business Plan" (PDF en 5 blocs)
2. API (Backend):
2.1. Endpoints documentés :
2.1.1. POST /api/upload → Reçoit le PDF, retourne un ID d’analyse
2.2.2. GET /api/results/{id} → Retourne les scores 5P + recommandations
2.2.3. POST /api/generate-pdf → Génère le business plan en PDF
- Documentation technique & utilisateur:
1. Utilisateur :
1.1. Fiche "Lire votre analyse 5P"
1.2. Tutoriel vidéo pour utiliser le dashboard
2. Technique :
2.1. Guide d'installation (variables d'environnement)
2.2. Swagger/OpenAPI pour l’API
    """


    team = {
        "sprint_length_days": 14,
        "project duration days": 14*4,
        "sprint_capacity_points": 20,
        "members": [
            {"name": "Sami", "role": "Backend", "skills": ["backend", "sql", "spring"]},
            {"name": "Moncef", "role": "Backend", "skills": ["backend", "sql", "spring"]},
            {"name": "Ali", "role": "Frontend", "skills": ["frontend", "angular", "ui"]},
            {"name": "Mouna", "role": "DevOps", "skills": ["devops", "docker", "ci_cd"]},
            {"name": "Hela", "role": "QA", "skills": ["qa", "testing","ml"]}
        ]
    }

    result = graph.invoke({
        "cahier_de_charge": cahier_de_charge,
        "team": team,
        "validation_attempts": 0,
        "max_validation_attempts": 1,
        "spec_validation_step": False
    })

    print(result)
    if(result["spec_validation"]["ok"]==False):
        print("validation failed")
        print(result["spec_validation"]["errors"])
        print(result["spec_fix_suggestions"])
    else:
        print("============================================")
        print("Requirements:", result["requirements"])
        print("============================================")
        print("Product Backlog:", result["product_backlog"])
        print("============================================")
        print("Estimated Backlog:", result["estimated_backlog"])
        print("============================================")
        print("Dependencies:", result.get("dependencies", []))
        print("============================================")
        print("Sprints:", result["sprint_backlogs"])
        print("============================================")
        print("Assignments:", result["assignments"])
        print("============================================")
        print("Validation:", result["validation"])
        print("============================================")

# --- Build chosen_fixes from the suggestion generator output ---
    chosen_fixes = [
        {
            "error_code": "REQUIREMENTS_TOO_HIGH_LEVEL",
            "severity": "medium",
            "message": "Les objectifs sont trop généraux et nécessitent une clarification",
            "evidence": "Les objectifs sont trop généraux et nécessitent une clarification",
            "fix_id": "S1",
            "title": "Définir des objectifs plus spécifiques et mesurables",
            "paragraph": "Les objectifs doivent être définis de manière plus spécifique et mesurable pour garantir que la plateforme PorterVision répond aux besoins réels des utilisateurs. Par exemple, l'objectif pourrait être de 'réduire le temps de prise de décision de 30% grâce à l'analyse automatique des données'."
        },
        {
            "error_code": "NFR_TOO_WEAK",
            "severity": "high",
            "message": "Les exigences non-fonctionnelles sont insuffisantes et nécessitent une complétion",
            "evidence": "Les exigences non-fonctionnelles sont insuffisantes et nécessitent une complétion",
            "fix_id": "S1",
            "title": "Définir des exigences non-fonctionnelles plus détaillées",
            "paragraph": "Les exigences non-fonctionnelles doivent être définies de manière plus détaillée pour garantir que la plateforme PorterVision répond aux besoins réels des utilisateurs. Par exemple, l'exigence non-fonctionnelle pourrait être 'la plateforme doit être accessible en ligne 24/7 avec une disponibilité de 99,9%'."
        },
        {
            "error_code": "OTHER",
            "severity": "medium",
            "message": "Le périmètre du projet est trop vague et nécessite une clarification",
            "evidence": "Le périmètre du projet est trop vague et nécessite une clarification",
            "fix_id": "S2",
            "title": "Définir des limites de projet",
            "paragraph": "Les limites de projet doivent être définies pour garantir que la plateforme PorterVision ne dépasse pas les ressources disponibles. Par exemple, les limites de projet pourraient inclure la définition des ressources humaines, des ressources matérielles, des budgets, etc."
        }
    ]

    result=merge_app.invoke({ "spec_original": cahier_de_charge, "chosen_fixes": chosen_fixes, "manual_patch": "" })
    print("Enhanced spec after merging chosen fixes:"+result["spec_enhanced"])

# 4️⃣ Fonction intelligente
def describe_image(image):
    # 4a️⃣ Essayer d'extraire du texte
    text_in_image = pytesseract.image_to_string(image).strip()

    if text_in_image:
        # Si du texte est détecté, le retourner
        return f"Texte détecté dans l'image :\n{text_in_image}"
    else:
        # Sinon, générer un paragraphe descriptif
        inputs = processor(images=image, return_tensors="pt")
        out = model.generate(**inputs, max_length=150)  # max_length pour plus de texte
        caption = processor.decode(out[0], skip_special_tokens=True)
        return f"Description complète de l'image :\n{caption}"



